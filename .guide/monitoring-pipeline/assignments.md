# Assignments

Complete the following assignments to reinforce the concepts we covered in this session. You don't have to work on every assignment. Pick the ones that you think will help you the most.

1. Extend the [Monitoring pipeline](src/pipelines/monitoring.py) to include a new step that checks for target drift between the reference dataset and the current production data. Target drift occurs when the distribution of the ground truth labels changes over time.

1. Modify the `start` step of the [Monitoring pipeline](src/pipelines/monitoring.py) to allow filtering production data by time (e.g. last 7 days vs last 30 days). This should be done by adding a new parameter to the pipeline and using it to filter the production data.

1. Modify the [Traffic pipeline](src/pipelines/traffic.py) with a new parameter to simulate drift in any of the features of production data. Generate new data with drift and ensure the [Monitoring pipeline](src/pipelines/monitoring.py) flags it.

1. Modify the `end` step of the [Monitoring pipeline](src/pipelines/monitoring.py) to serialize every JSON report generated by Evidently and save them to a versioned S3 location.

1. Modify the [Training pipeline](src/pipelines/training.py) with a new data validation step that checks for data quality issues like missing values, outliers, or data drift before training. The step should generate a data quality report and optionally fail the pipeline if quality metrics fall below thresholds.

1. Implement a new step in the [Monitoring pipeline](src/pipelines/monitoring.py) that uses the `TestAccuracyScore` test to verify that model accuracy remains above a configurable threshold (e.g., 80%). The step should fail the pipeline if accuracy drops below the threshold.

1. Create a custom monitoring report that combines data quality metrics from `DataQualityPreset` with classification metrics from `ClassificationPreset` into a single comprehensive dashboard view.

1. Implement a data freshness check by modifying the `start` step to validate that the production data timestamps are within an acceptable time range (e.g., no older than 7 days) and fail the pipeline if data is too stale.

1. Update the [Monitoring pipeline](src/pipelines/monitoring.py) with a new step that uses an LLM to generate a summary of the metrics generated by the pipeline. The goal is to use this summary to understand the performance of the pipeline at a glance.

1. Create a monitoring alerting system by extending the `end` step to send an email notification when critical tests fail, such as accuracy dropping below threshold or significant data drift detected.

