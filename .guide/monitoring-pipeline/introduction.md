# The Monitoring Pipeline

The Monitoring pipeline monitors the performance of a hosted model and the quality of the data it receives. It runs a series of tests and generates several reports using the data captured by the model and a reference dataset.

![Monitoring pipeline](.guide/monitoring-pipeline/images/monitoring.png)

Before we can run the Monitoring pipeline, we need to [generate some fake traffic](.guide/monitoring-pipeline/generating-fake-traffic.md) to the hosted model. The model will store the input data and the predictions it generates. We will then [generate ground truth labels](.guide/monitoring-pipeline/generating-fake-labels.md) for that data to evaluate the model's performance.

After generating the traffic and the labels, we can run the Monitoring pipeline to evaluate the quality of the data and the model's performance using the following recipe:

```shell
just monitor
```

The pipeline will load the reference and production datasets and generate a series of reports to evaluate the quality of the data and the model's performance. By default, the pipeline uses the `backend.Local` implementation to load the production data from the local SQLite database. You can change the [backend implementation](src/inference/backend.py) by specifying a different implementation using the `--backend` property:

```shell
uv run src/pipelines/monitoring.py run --backend backend.Local
```

To provide configuration settings to a specific backend implementation, you can use the `--config` parameter to supply a JSON configuration file to the pipeline. The [`config/local.json`](config/local.json) file is an example configuration file for the [`backend.Local`](src/inference/backend.py) backend. You can use this file as follows:

```shell
uv run src/pipelines/monitoring.py \
    --config config config/local.json run \
    --backend backend.Local
```

By default, the pipeline will load the latest 500 samples stored in the database and use them to generate the reports. You can change the number of samples to load by using the `--limit` parameter when running the pipeline:

```shell
uv run src/pipelines/monitoring.py run --limit 1000
```

After the pipeline finishes running, you can visualize the generated reports using Metaflow's built-in viewer. Run the card server using the command below, and navigate to [localhost:8324](http://localhost:8324). Every time you run the Monitoring pipeline, the viewer will automatically update to show the every report generated by the pipeline:

```shell
uv run src/pipelines/monitoring.py card server
```