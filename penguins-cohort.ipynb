{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b7ba7b-433c-463c-8e5e-8b975a5be463",
   "metadata": {},
   "source": [
    "# Penguins in Production\n",
    "\n",
    "This notebook aims to create a [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) to build an end-to-end Machine Learning system to solve the problem of classifying penguin species.\n",
    "\n",
    "This example uses the [Penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data).\n",
    "\n",
    "<img src='https://imgur.com/orZWHly.png' alt='Penguins dataset' width=\"900\">\n",
    "\n",
    "Amazon SageMaker is free to try. Your free tier starts from the first month you create your first SageMaker resource and lasts two months. Check out the  [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/) for more information. Also, we'll be working extensively with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) and the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/). Keep their documentation handy.\n",
    "\n",
    "This notebook was created by [Santiago L. Valdarrama](https://twitter.com/svpino) as part of the [Machine Learning School](https://www.ml.school) program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6203b0-5a40-4a14-b9bb-6e092f1bb2e7",
   "metadata": {},
   "source": [
    "Let's ensure we are running the latest version of the SakeMaker SDK. **Restart the Kernel** after you run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eb4984-3c21-4033-be45-2de2cfc1257c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mName: sagemaker\n",
      "Version: 2.146.0\n",
      "Summary: Open source library for training and deploying models on Amazon SageMaker.\n",
      "Home-page: https://github.com/aws/sagemaker-python-sdk/\n",
      "Author: Amazon Web Services\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /usr/local/lib/python3.8/site-packages\n",
      "Requires: attrs, boto3, google-pasta, importlib-metadata, jsonschema, numpy, packaging, pandas, pathos, platformdirs, protobuf, protobuf3-to-dict, PyYAML, schema, smdebug-rulesconfig\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade awscli boto3\n",
    "!pip install -q --upgrade sagemaker==2.146.0\n",
    "!pip show sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212a0538-62b5-4875-a59c-1a4e1a833a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86227bb7-0351-4d59-9576-c881130389f3",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "Let's start by preparing the S3 bucket where we will organize every resource we are going to use during the program. Make sure you set `BUCKET` to the bucket name you want to use. This name has to be unique. The [command line interface](https://docs.aws.amazon.com/cli/latest/index.html) is a simple way to interact with the AWS services. You can combine Python code with bash commands in the same notebook cell, which makes notebooks a very flexible tool.\n",
    "\n",
    "If you want to create a bucket in a region other than `us-east-1`, use this command instead:\n",
    "\n",
    "```\n",
    "!aws s3api create-bucket --bucket $BUCKET --create-bucket-configuration LocationConstraint=$region\n",
    "```\n",
    "\n",
    "The `LocationConstraint` argument should specify the region where you want to create the bucket.\n",
    "\n",
    "After we have a bucket, we can download the [Penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data) and store it in a folder inside the bucket. Our SageMaker Pipeline will use this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea8d2fd-7dd4-43c8-b9d9-5372448d72a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Location\": \"/mlschool\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "BUCKET = \"mlschool\"\n",
    "\n",
    "!aws s3api create-bucket --bucket $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882a8be9-dc31-4df1-9009-3b2d0284cb70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset S3 location: s3://mlschool/penguins/data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "import urllib.request\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PENGUINS_FOLDER = Path(\"penguins\")\n",
    "S3_FILEPATH = f\"s3://{BUCKET}/{PENGUINS_FOLDER}\"\n",
    "LOCAL_FILEPATH = Path(PENGUINS_FOLDER)/ \"data.csv\"\n",
    "\n",
    "# Create the local folder if it doesn't exist.\n",
    "PENGUINS_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the official Penguins dataset and store it locally.\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv\", \n",
    "    LOCAL_FILEPATH\n",
    ")\n",
    "\n",
    "# Upload the dataset to S3. We need to do this to make it available to \n",
    "# the preprocessing step.\n",
    "INPUT_DATA_URI = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=str(LOCAL_FILEPATH), \n",
    "    desired_s3_uri=S3_FILEPATH,\n",
    ")\n",
    "\n",
    "print(f\"Dataset S3 location: {INPUT_DATA_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1efbc-5f66-4b44-9555-68a4731e8e7b",
   "metadata": {},
   "source": [
    "We can now load and display the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19b546a-6884-483b-8cda-0521e2656a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0    Adelie  Torgersen              39.1             18.7              181.0   \n",
       "1    Adelie  Torgersen              39.5             17.4              186.0   \n",
       "2    Adelie  Torgersen              40.3             18.0              195.0   \n",
       "3    Adelie  Torgersen               NaN              NaN                NaN   \n",
       "4    Adelie  Torgersen              36.7             19.3              193.0   \n",
       "..      ...        ...               ...              ...                ...   \n",
       "339  Gentoo     Biscoe               NaN              NaN                NaN   \n",
       "340  Gentoo     Biscoe              46.8             14.3              215.0   \n",
       "341  Gentoo     Biscoe              50.4             15.7              222.0   \n",
       "342  Gentoo     Biscoe              45.2             14.8              212.0   \n",
       "343  Gentoo     Biscoe              49.9             16.1              213.0   \n",
       "\n",
       "     body_mass_g     sex  \n",
       "0         3750.0    MALE  \n",
       "1         3800.0  FEMALE  \n",
       "2         3250.0  FEMALE  \n",
       "3            NaN     NaN  \n",
       "4         3450.0  FEMALE  \n",
       "..           ...     ...  \n",
       "339          NaN     NaN  \n",
       "340       4850.0  FEMALE  \n",
       "341       5750.0    MALE  \n",
       "342       5200.0  FEMALE  \n",
       "343       5400.0    MALE  \n",
       "\n",
       "[344 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(LOCAL_FILEPATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992fc69-4868-4e50-a50e-76da237f70ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Session 1 - Preprocessing the Data\n",
    "\n",
    "This session aims to build a simple [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) with one step to preprocess the [Penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data). We'll use a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) with a [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-processor) to execute a preprocessing script. Check the [SageMaker Pipelines Overview](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) for an introduction to the fundamental components of a SageMaker Pipeline.\n",
    "\n",
    "Here is what the Pipeline will look like at the end of this session:\n",
    "\n",
    "<img src='penguins/images/session1-pipeline.png' alt='Session 1 Pipeline' width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "915b1d0b-d9da-4529-aca5-fd1c08a36f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tempfile\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CacheConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66efe2-6a01-41b5-ba8a-9c82c480994c",
   "metadata": {},
   "source": [
    "Let's start by defining a few variables we'll use throughout this notebook:\n",
    "\n",
    "* `sagemaker_client`: We'll use a [boto3 SageMaker Client](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) instance to access SageMaker.\n",
    "* `iam_client`: We'll use a [boto3 IAM Client](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html) instance to access IAM.\n",
    "* `role`: This is the execution role attached to this notebook. We can use this role with any of the SageMaker services that need it to ensure they run with the appropriate permissions.\n",
    "* `region`: The current region attached to our session. \n",
    "* `sagemaker_session`: The current SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7bb1e9bf-25d8-401b-a7d9-0455b1a7434d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam_client = boto3.client(\"iam\")\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d06e0-b711-4e3d-b424-6fa611a51f94",
   "metadata": {},
   "source": [
    "## Step 1 - Preprocessing the Dataset\n",
    "\n",
    "Let's create a script to do feature engineering on the original dataset. We will run this script using a SageMaker Processing Job later in this session.\n",
    "\n",
    "The script should split the data into train, validation, and test sets so we can later train and evaluate a model. We will save the Scikit-Learn pipeline that we use to preprocess the data to use it during inference time.\n",
    "\n",
    "The script uses the [np.split()](https://numpy.org/doc/stable/reference/generated/numpy.split.html) function to split the dataset into three sets in the following way:\n",
    "\n",
    "1. The train set will use the top 70% of the data.\n",
    "2. The validation set will use 15% of the data, starting with the sample after the 70% used for the train set.\n",
    "3. Finally, the test set will use the remaining 15% of the data.\n",
    "\n",
    "Pay special attention to the way the Scikit-Learn pipeline `preprocessor` is used to process the three sets:\n",
    "\n",
    "* First, we use the `fit_transform()` to fit the pipeline on the train set.\n",
    "* Then, we consecutively transform the validation and test sets using `transform()`.\n",
    "\n",
    "Always use `fit_transform()` on the training data to fit the scaling parameters we need to transform the data. For example, `fit_transform()` will learn the mean and variance of the features of the training set. It can then use these same parameters to scale the validation and test sets.\n",
    "\n",
    "That's why we want to save this Scikit-Learn pipeline to use later to scale production data using the same parameters we learned on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fb6ba7c0-1bd6-4fe5-8b7f-f6cbdfd3846c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PENGUINS_FOLDER}/preprocessor.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "# This is the location where the SageMaker Processing job\n",
    "# will save the input dataset.\n",
    "BASE_DIRECTORY = \"/opt/ml/processing\"\n",
    "DATA_FILEPATH = Path(BASE_DIRECTORY) / \"input\" / \"data.csv\"\n",
    "\n",
    "\n",
    "def _save_splits(base_directory, train, validation, test):\n",
    "    \"\"\"\n",
    "    One of the goals of this script is to output the three\n",
    "    dataset splits. This function will save each of these\n",
    "    splits to disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_path = Path(base_directory) / \"train\" \n",
    "    validation_path = Path(base_directory) / \"validation\" \n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "    \n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(validation_path / \"validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "    \n",
    "\n",
    "def _save_pipeline(base_directory, pipeline):\n",
    "    \"\"\"\n",
    "    Saves the Scikit-Learn pipeline that we used to\n",
    "    preprocess the data.\n",
    "    \"\"\"\n",
    "    pipeline_path = Path(base_directory) / \"pipeline\"\n",
    "    pipeline_path.mkdir(parents=True, exist_ok=True)\n",
    "    dump(pipeline, open(pipeline_path / \"pipeline.pkl\", 'wb'))\n",
    "    \n",
    "\n",
    "def _save_classes(base_directory, classes):\n",
    "    \"\"\"\n",
    "    Saves the list of classes from the dataset.\n",
    "    \"\"\"\n",
    "    path = Path(base_directory) / \"classes\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    np.asarray(classes).tofile(path / \"classes.csv\", sep = \",\") \n",
    "    \n",
    "\n",
    "def _generate_baseline_dataset(split_name, base_directory, X, y):\n",
    "    \"\"\"\n",
    "    To monitor the data and the quality of our model we need to compare the \n",
    "    production quality and results against a baseline. To create those baselines, \n",
    "    we need to use a dataset to compute statistics and constraints. That dataset\n",
    "    should contain information in the same format as expected by the production\n",
    "    endpoint. This function will generate a baseline dataset and save it to \n",
    "    disk so we can later use it.\n",
    "    \n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / f\"{split_name}-baseline\" \n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = X.copy()\n",
    "    \n",
    "    # The baseline dataset needs a column containing the groundtruth.\n",
    "    df[\"groundtruth\"] = y\n",
    "    df[\"groundtruth\"] = df[\"groundtruth\"].values.astype(str)\n",
    "    \n",
    "    # We will use the baseline dataset to generate baselines\n",
    "    # for monitoring data and model quality. To simplify the process, \n",
    "    # we don't want to include any NaN rows.\n",
    "    df = df.dropna()\n",
    "\n",
    "    df.to_json(baseline_path / f\"{split_name}-baseline.json\", orient='records', lines=True)\n",
    "    \n",
    "    \n",
    "def preprocess(base_directory, data_filepath):\n",
    "    \"\"\"\n",
    "    Preprocesses the supplied raw dataset and splits it into a train, validation,\n",
    "    and a test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_filepath)\n",
    "    \n",
    "    numerical_columns = [column for column in df.columns if df[column].dtype in [\"int64\", \"float64\"]]\n",
    "    \n",
    "    numerical_preprocessor = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_preprocessor = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"numerical\", numerical_preprocessor, numerical_columns),\n",
    "            (\"categorical\", categorical_preprocessor, [\"island\"]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    X = df.drop([\"sex\"], axis=1)\n",
    "    columns = list(X.columns)\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    \n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(.7 * len(X)), int(.85 * len(X))])\n",
    "    \n",
    "    X_train = pd.DataFrame(train, columns=columns)\n",
    "    X_validation = pd.DataFrame(validation, columns=columns)\n",
    "    X_test = pd.DataFrame(test, columns=columns)\n",
    "    \n",
    "    y_train = X_train.species\n",
    "    y_validation = X_validation.species\n",
    "    y_test = X_test.species\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_validation = label_encoder.transform(y_validation)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    X_train.drop([\"species\"], axis=1, inplace=True)\n",
    "    X_validation.drop([\"species\"], axis=1, inplace=True)\n",
    "    X_test.drop([\"species\"], axis=1, inplace=True)\n",
    "\n",
    "    # Let's generate a dataset that we can later use to compute\n",
    "    # baseline statistics and constraints about the data that we\n",
    "    # used to train our model.\n",
    "    _generate_baseline_dataset(\"train\", base_directory, X_train, y_train)\n",
    "    \n",
    "    # To generate baseline constraints about the quality of the\n",
    "    # model's predictions, we will use the test set.\n",
    "    _generate_baseline_dataset(\"test\", base_directory, X_test, y_test)\n",
    "    \n",
    "    # Transform the data using the Scikit-Learn pipeline.\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_validation = preprocessor.transform(X_validation)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    \n",
    "    train = np.concatenate((X_train, np.expand_dims(y_train, axis=1)), axis=1)\n",
    "    validation = np.concatenate((X_validation, np.expand_dims(y_validation, axis=1)), axis=1)\n",
    "    test = np.concatenate((X_test, np.expand_dims(y_test, axis=1)), axis=1)\n",
    "    \n",
    "    _save_splits(base_directory, train, validation, test)\n",
    "    _save_pipeline(base_directory, pipeline=preprocessor)\n",
    "    _save_classes(base_directory, label_encoder.classes_)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(BASE_DIRECTORY, DATA_FILEPATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214fbdcb-8ee8-4104-bcd3-d7329693299e",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Preprocessing Script\n",
    "\n",
    "We can now load the script we just created and run it locally to ensure it outputs every file we need.\n",
    "\n",
    "We will set up a SageMaker Processing Job to run this script, but we always want to test the code locally. In this case, we can call the `preprocess()` function with the local directory and the local copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d1f122a4-acff-4687-91b9-bfef13567d88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders: ['train-baseline', 'test-baseline', 'train', 'validation', 'test', 'pipeline', 'classes']\n"
     ]
    }
   ],
   "source": [
    "from penguins.preprocessor import preprocess\n",
    "\n",
    "\n",
    "def print_baseline(split_name):\n",
    "    print()\n",
    "    print(f\"Baseline {split_name}:\")\n",
    "    with open(Path(directory) / f\"{split_name}-baseline\" / f\"{split_name}-baseline.json\") as baseline:\n",
    "        lines = [next(baseline) for _ in range(5)]\n",
    "        \n",
    "    for l in lines:\n",
    "        print(l[:-1])\n",
    "    \n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    preprocess(\n",
    "        base_directory=directory, \n",
    "        data_filepath=LOCAL_FILEPATH\n",
    "    )\n",
    "    \n",
    "    print(f\"Folders: {os.listdir(directory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3938a1d-d5bd-4967-ad65-a945d5df1f61",
   "metadata": {},
   "source": [
    "## Step 3 - Pipeline Configuration\n",
    "\n",
    "When creating a SageMaker Pipeline, we can specify a list of parameters we can use on individual pipeline steps. To read more about these parameters, check [Pipeline Parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html).\n",
    "\n",
    "These are the parameters that we need right now:\n",
    "\n",
    "* `dataset_location`: This parameter represents the dataset's location in S3. We will use this parameter to indicate the SageMaker Processing Job where to find the dataset. The Processing Job will download the dataset from S3 and make it available on the instance running the script.\n",
    "* `preprocessor_destination`: We need to define the location where the SageMaker Processing Job will store the output. When it finishes, the Processing Job will copy the script's output to the S3 location specified by this parameter. By default, SageMaker uploads the output of a job to a custom location in S3, but unfortunately, if we rely on that functionality, we can't cache the Processing Step in the Pipeline.\n",
    "* `train_dataset_baseline_destination`: This parameter represents the location where we will store the train dataset to compute constraints and statistic baselines in Session 6.\n",
    "* `test_dataset_baseline_destination`: This parameter represents the location where we will store the test dataset to compute constraints and statistic baselines in Session 6.\n",
    "* `timestamp_signature`: We'll use this parameter to automatically generate resources using a unique suffix to avoid collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "86de7edd-18b0-40d1-ac8e-0f3ef4469be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=INPUT_DATA_URI,\n",
    ")\n",
    "\n",
    "preprocessor_destination = ParameterString(\n",
    "    name=\"preprocessor_destination\",\n",
    "    default_value=f\"{S3_FILEPATH}/preprocessing\",\n",
    ")\n",
    "\n",
    "train_dataset_baseline_destination = ParameterString(\n",
    "    name=\"train_dataset_baseline_destination\",\n",
    "    default_value=f\"{S3_FILEPATH}/preprocessing/baselines/train\",\n",
    ")\n",
    "\n",
    "test_dataset_baseline_destination = ParameterString(\n",
    "    name=\"test_dataset_baseline_destination\",\n",
    "    default_value=f\"{S3_FILEPATH}/preprocessing/baselines/test\",\n",
    ")\n",
    "\n",
    "timestamp_signature = ParameterString(\n",
    "    name=\"timestamp_signature\",\n",
    "    default_value=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b42440-b3c0-4ded-b578-52e21f5fbfdd",
   "metadata": {},
   "source": [
    "## Step 4 - Caching Pipeline Steps\n",
    "\n",
    "While building a pipeline, you only want to rerun every step if you expect a different result. To accomplish this, you can instruct SageMaker to reuse the result of a previous successful run of a pipeline step. You can find more information about this topic in [Caching Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c7e0a332-e54e-4e57-97c7-9b54f4058738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_config = CacheConfig(\n",
    "    enable_caching=True, \n",
    "    expire_after=\"15d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9771e08-5560-492a-845e-f06988b6ae1b",
   "metadata": {},
   "source": [
    "## Step 5 - Setting up a Processing Step\n",
    "\n",
    "The first step we need in the pipeline is a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) to run the preprocessing script. This Processing Step will create a SageMaker Processing Job in the background, run the script, and upload the output to S3. You can use Processing Jobs to perform data preprocessing, post-processing, feature engineering, data validation, and model evaluation. Check the [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "A processor gives the Processing Step information about the hardware and software that SageMaker should use to launch the Processing Job. To run the script, we need access to Scikit-Learn, so we can use the [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-processor) processor that comes out-of-the-box with the SageMaker's Python SDK. The [Data Processing with Framework Processors](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job-frameworks.html) page discusses other built-in processors you can use. The [Docker Registry Paths and Example Code](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html) page contains information about the available framework versions for each region.\n",
    "\n",
    "The [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) requires a list of inputs that we need on the preprocessing script. In this case, the input is the dataset we stored in S3. We also have a few outputs that we want SageMaker to capture when the Processing Job finishes. SageMaker will upload every one of these outputs to the location specified by the `preprocessor_destination` parameter except the baseline data, which we will upload to the location specified by the `baseline_destination` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2187b65e-e504-40a5-ad05-82f54885805c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name=\"penguins-preprocessing\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "preprocess_data_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=dataset_location, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"pipeline\", source=\"/opt/ml/processing/pipeline\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"classes\", source=\"/opt/ml/processing/classes\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"train-baseline\", source=\"/opt/ml/processing/train-baseline\", destination=train_dataset_baseline_destination),\n",
    "        ProcessingOutput(output_name=\"test-baseline\", source=\"/opt/ml/processing/test-baseline\", destination=test_dataset_baseline_destination),\n",
    "    ],\n",
    "    code=f\"{PENGUINS_FOLDER}/preprocessor.py\",\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465690a-67b5-4d68-a026-dd874ee569e8",
   "metadata": {},
   "source": [
    "## Step 6 - Running the Pipeline\n",
    "\n",
    "Let's define and run the SageMaker Pipeline. Check [Pipeline Structure and Execution](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-pipeline.html) for more information about how to define a pipeline and [Run a Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/run-pipeline.html) for information about how to run it.\n",
    "\n",
    "The pipeline uses the parameters we defined before and the Preprocess Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "564fe6fb-00aa-480f-a58f-3144b2e33a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session1_pipeline = Pipeline(\n",
    "    name=\"penguins-session1-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "        train_dataset_baseline_destination,\n",
    "        test_dataset_baseline_destination\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d0ac8-1439-4329-9a1c-c43eafacd0da",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f4410f2f-a17b-4272-abd2-e092e134174c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session1_pipeline.upsert(role_arn=role)\n",
    "execution = session1_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c190c5-52b5-4ccc-8d42-847a694b8e66",
   "metadata": {},
   "source": [
    "# Session 2 - Model Training and Tuning\n",
    "\n",
    "This session extends the [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) we built in the previous session with a step to train a model. We'll explore the [Training](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) and the [Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning) steps. \n",
    "\n",
    "Here is what the Pipeline will look like at the end of this session:\n",
    "\n",
    "<img src='penguins/images/session2-pipeline.png' alt='Session 2 Pipeline' width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4756a3d8-d47b-4a88-a7f2-65ed7d4c1175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.parameter import IntegerParameter\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8608092-7aab-4fd2-aa99-47c2db27bdb7",
   "metadata": {},
   "source": [
    "## Step 1 - Training the Model\n",
    "\n",
    "This script is responsible for training a simple neural network on the train data, validating the model, and saving it so we can later use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d92b121d-dcb9-43e8-9ee3-3ececb583e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PENGUINS_FOLDER}/train.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def train(base_directory, train_path, validation_path, epochs=50, batch_size=32):\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(10, input_shape=(X_train.shape[1],), activation=\"relu\"),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        validation_data=(X_validation, y_validation),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    predictions = np.argmax(model.predict(X_validation), axis=-1)\n",
    "    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n",
    "    \n",
    "    model_filepath = Path(base_directory) / \"model\" / \"001\"\n",
    "    model.save(model_filepath)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Any hyperparameters provided by the training job are passed to the entry point\n",
    "    # as script arguments. SageMaker will also provide a list of special parameters\n",
    "    # that you can capture here. Here is the full list: \n",
    "    # https://github.com/aws/sagemaker-training-toolkit/blob/master/src/sagemaker_training/params.py\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--base_directory\", type=str, default=\"/opt/ml/\")\n",
    "    parser.add_argument(\"--train_path\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\", None))\n",
    "    parser.add_argument(\"--validation_path\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\", None))\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    train(\n",
    "        base_directory=args.base_directory,\n",
    "        train_path=args.train_path,\n",
    "        validation_path=args.validation_path,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0a4fa-ce70-4882-b9f5-8253df03d890",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Training Script\n",
    "\n",
    "Let's test the script we just created by running it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14ea27ce-c453-4cb0-b309-dbecd732957e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Extension horovod.torch has not been built: /usr/local/lib/python3.8/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-38-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2023-06-06 12:51:03.388 tensorflow-2-6-cpu-py-ml-t3-medium-9169b2e75617c45c79c40579f6a8:66 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-06-06 12:51:03.535 tensorflow-2-6-cpu-py-ml-t3-medium-9169b2e75617c45c79c40579f6a8:66 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "8/8 - 1s - loss: 1.0105 - accuracy: 0.5523 - val_loss: 0.9449 - val_accuracy: 0.7255\n",
      "Epoch 2/10\n",
      "8/8 - 0s - loss: 0.9615 - accuracy: 0.6485 - val_loss: 0.9040 - val_accuracy: 0.8039\n",
      "Epoch 3/10\n",
      "8/8 - 0s - loss: 0.9167 - accuracy: 0.7238 - val_loss: 0.8668 - val_accuracy: 0.8824\n",
      "Epoch 4/10\n",
      "8/8 - 0s - loss: 0.8745 - accuracy: 0.7824 - val_loss: 0.8350 - val_accuracy: 0.8824\n",
      "Epoch 5/10\n",
      "8/8 - 0s - loss: 0.8391 - accuracy: 0.8285 - val_loss: 0.8066 - val_accuracy: 0.8627\n",
      "Epoch 6/10\n",
      "8/8 - 0s - loss: 0.8073 - accuracy: 0.8452 - val_loss: 0.7806 - val_accuracy: 0.8431\n",
      "Epoch 7/10\n",
      "8/8 - 0s - loss: 0.7784 - accuracy: 0.8494 - val_loss: 0.7568 - val_accuracy: 0.8431\n",
      "Epoch 8/10\n",
      "8/8 - 0s - loss: 0.7515 - accuracy: 0.8494 - val_loss: 0.7336 - val_accuracy: 0.8431\n",
      "Epoch 9/10\n",
      "8/8 - 0s - loss: 0.7257 - accuracy: 0.8452 - val_loss: 0.7107 - val_accuracy: 0.8431\n",
      "Epoch 10/10\n",
      "8/8 - 0s - loss: 0.7004 - accuracy: 0.8494 - val_loss: 0.6888 - val_accuracy: 0.8431\n",
      "Validation accuracy: 0.8431372549019608\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp5sek_tt4/model/001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5sek_tt4/model/001/assets\n"
     ]
    }
   ],
   "source": [
    "from penguins.preprocessor import preprocess\n",
    "from penguins.train import train\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    # First, we preprocess the data and create the \n",
    "    # dataset splits.\n",
    "    preprocess(\n",
    "        base_directory=directory, \n",
    "        data_filepath=LOCAL_FILEPATH\n",
    "    )\n",
    "\n",
    "    # Then, we train a model using the train and \n",
    "    # validation splits.\n",
    "    train(\n",
    "        base_directory=directory, \n",
    "        train_path=Path(directory) / \"train\", \n",
    "        validation_path=Path(directory) / \"validation\",\n",
    "        epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cff4c1-6510-4d99-8ae1-cb14927b87c7",
   "metadata": {},
   "source": [
    "## Step 3 - Setting up a Training Step\n",
    "\n",
    "We can now create a [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) that we can add to the pipeline. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the [TrainingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TrainingStep) SageMaker's SDK documentation for more information. \n",
    "\n",
    "SageMaker uses the concept of an [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to handle end-to-end training and deployment tasks. For this example, we will use the built-in [TensorFlow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator) to run the training script we wrote before. The [Docker Registry Paths and Example Code](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html) page contains information about the available framework versions for each region. Here, you can also check the available SageMaker [Deep Learning Container images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).\n",
    "\n",
    "Notice the list of hyperparameters defined below. SageMaker will pass these hyperparameters as arguments to the entry point of the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90fe82ae-6a2c-4461-bc83-bb52d8871e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "    entry_point=f\"{PENGUINS_FOLDER}/train.py\",\n",
    "    \n",
    "    hyperparameters={\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    \n",
    "    framework_version=\"2.6\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    script_mode=True,\n",
    "    \n",
    "    disable_profiler=True,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d2b43-3bb5-4fe9-b3e4-cb8eb55c8a21",
   "metadata": {},
   "source": [
    "We can now create the [TrainingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TrainingStep) using the estimator we defined before.\n",
    "\n",
    "This step will receive the train and validation split from the preprocessing step as inputs. Notice how we reference both splits using the `preprocess_data_step` variable. This creates a dependency between the Training and Processing Step we defined in Session 1. When we build a new Pipeline, we'll see that the Training Step will run once the Processing Step finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99e4850c-83d6-4f4e-a813-d5a3f4bb7486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_model_step = TrainingStep(\n",
    "    name=\"train-model\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814e258-c633-4e9a-85c5-6ed0f168b503",
   "metadata": {},
   "source": [
    "## Step 4 - Setting up a Tuning Step\n",
    "\n",
    "Let's now create a [Tuning Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning) to add it to our pipeline. This Tuning Step will create a SageMaker Hyperparameter Tuning Job in the background and use the training script to train different model variants and choose the best one. Check the [TuningStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "The Tuning Step requires a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) reference to configure the Hyperparameter Tuning Job. In this example, the tuner will use the same `Estimator` we defined to train the model.\n",
    "\n",
    "Here is the configuration that we'll use to find the best model:\n",
    "\n",
    "1. `objective_metric_name`: This is the name of the metric the tuner will use to determine the best model.\n",
    "2. `objective_type`: This is the objective of the tuner. Should it \"Minimize\" the metric or \"Maximize\" it? In this example, since we are using the validation accuracy of the model, we want the objective to be \"Maximize.\" If we were using the loss of the model, we would set the objective to \"Minimize.\"\n",
    "3. `metric_definitions`: Defines how the tuner will determine the metric's value by looking at the output logs of the training process.\n",
    "\n",
    "The tuner expects the list of the hyperparameters you want to explore. You can use subclasses of the [Parameter](https://sagemaker.readthedocs.io/en/stable/api/training/parameter.html#sagemaker.parameter.ParameterRange) class to specify different types of hyperparameters. This example explores different values for the `epochs` hyperparameter.\n",
    "\n",
    "Finally, you can control the number of jobs and how many of them will run in parallel using the following two arguments:\n",
    "\n",
    "* `max_jobs`: Defines the maximum total number of training jobs to start for the hyperparameter tuning job.\n",
    "* `max_parallel_jobs`: Defines the maximum number of parallel training jobs to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "038ff2e5-ed28-445b-bc03-4e996ec2286f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objective_metric_name = \"val_accuracy\"\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [{\"Name\": objective_metric_name, \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}]\n",
    "    \n",
    "hyperparameter_ranges = {\n",
    "    \"epochs\": IntegerParameter(10, 50),\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    objective_type=objective_type,\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de0743-6c9e-4895-b060-f58a6d60c50d",
   "metadata": {},
   "source": [
    "We can now create the [TuningStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep). \n",
    "\n",
    "This step will use the tuner we configured before and will receive the train and validation split from the preprocessing step as inputs. Notice how we reference both splits using the `preprocess_data_step` variable. This creates a dependency between the Tuning and Processing Steps we defined in Session 1. When we build a new Pipeline, we'll see that the Tuning Step will run once the Processing Step finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf5dd9a7-8643-4fbb-8eb4-40f39011e27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tune_model_step = TuningStep(\n",
    "    name = \"tune-model\",\n",
    "    tuner=tuner,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c9362-ef17-4d68-b8c8-cefe21326ba2",
   "metadata": {},
   "source": [
    "## Step 5 - Switching Between Training and Tuning\n",
    "\n",
    "We could use a [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) or use a [Tuning Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning) to create the model.\n",
    "\n",
    "In this notebook, we will alternate between both methods and use the `USE_TUNING_STEP` flag to indicate which approach we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3df0f7a-e43a-4a7b-9809-0a639fa178d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_TUNING_STEP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babe38c-1682-42d2-8442-101d17aa89b5",
   "metadata": {},
   "source": [
    "## Step 6 - Running the Pipeline\n",
    "\n",
    "We can now define and run the SageMaker Pipeline using the Training or Tuning Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9799ab39-fcae-41f4-a68b-85ab71b3ba9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session2_pipeline = Pipeline(\n",
    "    name=\"penguins-session2-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "        train_dataset_baseline_destination,\n",
    "        test_dataset_baseline_destination,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0708c-3692-4d09-a269-bbd14f7b0226",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77c6d523-6810-4bc7-8fc4-64ae978cf245",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "session2_pipeline.upsert(role_arn=role)\n",
    "execution = session2_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d40fe8-ba74-4c12-9555-d8ea33d1c8b4",
   "metadata": {},
   "source": [
    "# Session 3 - Model Registration\n",
    "\n",
    "This session extends the [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) with a step to evaluate the model and a step to register a new model if it reaches a predefined accuracy threshold. We'll use a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) with a [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor) running TensorFlow to execute an evaluation script. We'll use a [Condition Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition) to determine whether the model's accuracy is above a threshold and a [Model Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-model) to register the model. After we register the model, we'll deploy it manually. To learn more about the Model Registry, check [Register and Deploy Models with Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html).\n",
    "\n",
    "Here is what the Pipeline will look like at the end of this session:\n",
    "\n",
    "<img src='penguins/images/session3-pipeline.png' alt='Session 3 Pipeline' width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34fc262f-a1cf-4f94-9c60-e7c8e83cfdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tarfile\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.properties import PropertyFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa9691-f49f-48af-b272-3d4d17563b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - Evaluating the Model\n",
    "\n",
    "This script is responsible for loading the model we created and evaluating it on the test set. Before finishing, this script will generate an evaluation report of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ee3ab26-afa5-4ceb-9f7a-005d5fdea646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PENGUINS_FOLDER}/evaluation.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/opt/ml/processing/model/\"\n",
    "TEST_PATH = \"/opt/ml/processing/test/\"\n",
    "OUTPUT_PATH = \"/opt/ml/processing/evaluation/\"\n",
    "\n",
    "\n",
    "def evaluate(model_path, test_path, output_path):\n",
    "    # The first step is to extract the model package provided\n",
    "    # by SageMaker.\n",
    "    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n",
    "        tar.extractall(path=Path(model_path))\n",
    "        \n",
    "    # We can now load the model from disk.\n",
    "    model = keras.models.load_model(Path(model_path) / \"001\")\n",
    "    \n",
    "    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test.drop(X_test.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "    # Let's add the accuracy of the model to our evaluation report.\n",
    "    evaluation_report = {\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": accuracy\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # We need to save the evaluation report to the output path.\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n",
    "        f.write(json.dumps(evaluation_report))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate(\n",
    "        model_path=MODEL_PATH, \n",
    "        test_path=TEST_PATH,\n",
    "        output_path=OUTPUT_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc79a0-adfd-4ce9-8580-5cd228c3c2d9",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Evaluation Script\n",
    "\n",
    "Let's test the script we just created by running it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9a2540d8-278a-4953-bc54-0469d154427d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 - 1s - loss: 1.1152 - accuracy: 0.1255 - val_loss: 1.0982 - val_accuracy: 0.1569\n",
      "Epoch 2/10\n",
      "8/8 - 0s - loss: 1.0670 - accuracy: 0.2678 - val_loss: 1.0614 - val_accuracy: 0.2941\n",
      "Epoch 3/10\n",
      "8/8 - 0s - loss: 1.0252 - accuracy: 0.3933 - val_loss: 1.0291 - val_accuracy: 0.3922\n",
      "Epoch 4/10\n",
      "8/8 - 0s - loss: 0.9881 - accuracy: 0.4686 - val_loss: 0.9997 - val_accuracy: 0.4510\n",
      "Epoch 5/10\n",
      "8/8 - 0s - loss: 0.9538 - accuracy: 0.5272 - val_loss: 0.9720 - val_accuracy: 0.4706\n",
      "Epoch 6/10\n",
      "8/8 - 0s - loss: 0.9214 - accuracy: 0.5565 - val_loss: 0.9457 - val_accuracy: 0.4902\n",
      "Epoch 7/10\n",
      "8/8 - 0s - loss: 0.8913 - accuracy: 0.6151 - val_loss: 0.9202 - val_accuracy: 0.5294\n",
      "Epoch 8/10\n",
      "8/8 - 0s - loss: 0.8630 - accuracy: 0.6402 - val_loss: 0.8958 - val_accuracy: 0.5490\n",
      "Epoch 9/10\n",
      "8/8 - 0s - loss: 0.8354 - accuracy: 0.6736 - val_loss: 0.8721 - val_accuracy: 0.6275\n",
      "Epoch 10/10\n",
      "8/8 - 0s - loss: 0.8093 - accuracy: 0.7238 - val_loss: 0.8498 - val_accuracy: 0.6667\n",
      "Validation accuracy: 0.6666666666666666\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpmevl6m37/model/001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmevl6m37/model/001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff83f295820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff83f295820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7254901960784313\n"
     ]
    }
   ],
   "source": [
    "from penguins.preprocessor import preprocess\n",
    "from penguins.train import train\n",
    "from penguins.evaluation import evaluate\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    # First, we preprocess the data and create the \n",
    "    # dataset splits.\n",
    "    preprocess(\n",
    "        base_directory=directory, \n",
    "        data_filepath=LOCAL_FILEPATH\n",
    "    )\n",
    "\n",
    "    # Then, we train a model using the train and \n",
    "    # validation splits.\n",
    "    train(\n",
    "        base_directory=directory, \n",
    "        train_path=Path(directory) / \"train\", \n",
    "        validation_path=Path(directory) / \"validation\",\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    # After training a model, we need to prepare a package just like\n",
    "    # SageMaker would. This package is what the evaluation script is\n",
    "    # expecting as an input.\n",
    "    with tarfile.open(Path(directory) / \"model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(Path(directory) / \"model\" / \"001\", arcname=\"001\")\n",
    "        \n",
    "    \n",
    "    # We can now call the evaluation script.\n",
    "    evaluate(\n",
    "        model_path=directory, \n",
    "        test_path=Path(directory) / \"test\",\n",
    "        output_path=Path(directory) / \"evaluation\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1109a-6c26-4464-8338-94960729d212",
   "metadata": {},
   "source": [
    "## Step 3 - Setting up a Processing Step\n",
    "\n",
    "To run the evaluation script, we can use a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing). Check the [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "Whenever you want to run a Processing Job using a machine learning framework, you can use an instance of the [FrameworkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.FrameworkProcessor) class. For example, the [TensorFlowProcessor](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job-frameworks-tensorflow.html) subclass will give you access to TensorFlow. You can also configure a Processing Job from scratch using a [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor) instance combined with the [sagemaker.image_uris.retrieve()](https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html) function for generating the URI of one of the SageMaker pre-built docker images. This time, we will use a [TensorFlowProcessor](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job-frameworks-tensorflow.html) because we need our script to have access to TensorFlow and Scikit-Learn.\n",
    "\n",
    "The inputs of this Processing Step will be the model we created and the test set we generated during the preprocessing phase. The output will be the evaluation report file.\n",
    "\n",
    "At this point, we create a model using either a Training Step or a Tuning Step, so we can use the `USE_TUNING_STEP` flag to configure the input to the Processing Step. In case we are using the Tuning Step, we can use the [TuningStep.get_top_model_s3_uri()](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep.get_top_model_s3_uri) function to get the model artifacts from the top performing training job of the Hyperparameter Tuning Job.\n",
    "\n",
    "The [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) lets us specify a list of [PropertyFile](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.properties.PropertyFile) instances from the output of the job. We can use this to map the evaluation report generated in the evaluation script. Check [How to Build and Manage Property Files](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html) for more information.\n",
    "\n",
    "We also need to define a new Pipeline parameter with the location where the Processing Step will store the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "48139a07-5c8e-4bc6-b666-bf9531f7f520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "tensorflow_processor = TensorFlowProcessor(\n",
    "    framework_version=\"2.6\",\n",
    "    py_version=\"py38\",\n",
    "    base_job_name=\"penguins-evaluation-processor\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "# By default, the TensorFlowProcessor runs the script using\n",
    "# /bin/bash as its entrypoint. We want to ensure we run it \n",
    "# using python3.\n",
    "tensorflow_processor.framework_entrypoint_command = [\"python3\"]\n",
    "\n",
    "\n",
    "# We want to map the evaluation report that we generate inside\n",
    "# the evaluation script so we can later reference it.\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"evaluation-report\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "\n",
    "# Notice how this step uses the model generated by the tuning or training\n",
    "# step, and the test set generated by the preprocessing step.\n",
    "evaluate_model_step = ProcessingStep(\n",
    "    name=\"evaluate-model\",\n",
    "    processor=tensorflow_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=(\n",
    "                tune_model_step.get_top_model_s3_uri(top_k=0, s3_bucket=sagemaker_session.default_bucket()) \n",
    "                if USE_TUNING_STEP \n",
    "                else train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "            ),\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\", destination=f\"{S3_FILEPATH}/evaluation\"),\n",
    "    ],\n",
    "    code=f\"{PENGUINS_FOLDER}/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a48e6-507d-42fb-8abc-ebde997e3d43",
   "metadata": {},
   "source": [
    "## Step 4 - Configuring the Model Metrics\n",
    "\n",
    "When we register a model, we can specify a set of [ModelMetrics](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_metrics.ModelMetrics). We can use the evaluation report we generated during the Evaluation step to populate these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4982146f-0c0f-4938-b5d0-06db45a58531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(on=\"/\", values=[\n",
    "            evaluate_model_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri'],\n",
    "            \"evaluation.json\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fac3a-5ecc-441f-84c3-717c4c7ba290",
   "metadata": {},
   "source": [
    "## Step 5 - Registering the Model\n",
    "\n",
    "We can now create a [Model Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-model) to register the model. Check the [ModelStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.model_step.ModelStep) SageMaker's SDK documentation for more information. We aim to create a new version of the model and register it in the Model Registry. Check [Register a Model Version](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-version.html) for more information about model registration.\n",
    "\n",
    "The model we trained uses TensorFlow, so we can use the built-in [TensorFlowModel](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-serving-model) class to create an instance of the model.\n",
    "\n",
    "Notice that we use an instance of the [PipelineSession](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline_context.PipelineSession) class to create the model. This special session does not register the model immediately when you call `model.register()`, instead, it captures the arguments required to register a model, and delegate it to the `ModelStep` to register the model later during pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "52a48cef-fb78-412b-a5c6-977eafe98e27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.tensorflow.model:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "/usr/local/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py:270: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = \"penguins\"\n",
    "\n",
    "model = TensorFlowModel(\n",
    "    model_data=(\n",
    "        tune_model_step.get_top_model_s3_uri(top_kabs=0, s3_bucket=sagemaker_session.default_bucket())\n",
    "        if USE_TUNING_STEP\n",
    "        else train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "    ),\n",
    "    framework_version=\"2.6\",\n",
    "    sagemaker_session=PipelineSession(),\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "register_model_step = ModelStep(\n",
    "    name=\"register-model\",\n",
    "    step_args=model.register(\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        model_metrics=model_metrics,\n",
    "        approval_status=\"Approved\",\n",
    "        \n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"TENSORFLOW\",\n",
    "        framework_version=\"2.6\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c110f7-fe72-4db8-9d06-cfb9a0f2bfbd",
   "metadata": {},
   "source": [
    "## Step 6 - Setting up a Condition Step\n",
    "\n",
    "We only want to register a new model if its accuracy exceeds a predefined threshold. We can use a [Condition Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition) together with the evaluation report we generated in the Evaluation step to accomplish this. Check the [ConditionStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#conditionstep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "In this example, we will use a [ConditionGreaterThanOrEqualTo](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.conditions.ConditionGreaterThanOrEqualTo) condition to compare the model's accuracy with the threshold. Look at the [Conditions](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html#conditions) section in the documentation for more information about the types of supported conditions.\n",
    "\n",
    "If the model's accuracy is not greater than or equal our threshold, we will send the pipeline to a [Fail Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-fail) with the appropriate error message. Check the [FailStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.fail_step.FailStep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "We are going to use a new [Pipeline Parameter](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html) in our pipeline to specify the minimum accuracy that the model should reach for it to be registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36e2a2b1-6711-4266-95d8-d2aebd52e199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_threshold = ParameterFloat(\n",
    "    name=\"accuracy_threshold\", \n",
    "    default_value=0.70\n",
    ")\n",
    "\n",
    "condition_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluate_model_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\"\n",
    "    ),\n",
    "    right=accuracy_threshold\n",
    ")\n",
    "\n",
    "fail_step = FailStep(\n",
    "    name=\"fail\",\n",
    "    error_message=Join(\n",
    "        on=\" \", \n",
    "        values=[\n",
    "            \"Execution failed because the model's accuracy was lower than\", \n",
    "            accuracy_threshold\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "condition_step = ConditionStep(\n",
    "    name=\"check-model-accuracy\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[register_model_step],\n",
    "    else_steps=[fail_step], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309b8fa-f03e-4959-853f-dc2416f82bdd",
   "metadata": {},
   "source": [
    "## Step 7 - Running the Pipeline\n",
    "\n",
    "We can now add the registration of the model to the pipeline. Notice how we add the Condition Step, which will call the Model Step if the condition passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f70bcd33-b499-4e2b-953e-94d1ed96c10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session3_pipeline = Pipeline(\n",
    "    name=\"penguins-session3-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "        train_dataset_baseline_destination,\n",
    "        test_dataset_baseline_destination,\n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step, \n",
    "        evaluate_model_step,\n",
    "        condition_step\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e98fb1-fdff-4ba4-aea3-0d9f271fb24c",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4864ab08-7acc-4719-970e-fcce66c5fef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "session3_pipeline.upsert(role_arn=role)\n",
    "execution = session3_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cf77e-7fc7-406e-a2e2-40c553f459f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Session 4 - Model Deployment\n",
    "\n",
    "This session extends the [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) with a step to deploy the model to an endpoint. We'll use a [Lambda Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-lambda) to create an endpoint and deploy the model. To control the endpoint's inputs and outputs, we'll modify the model's assets to include code that customizes the processing of a request. \n",
    "\n",
    "At the end of this session, our Pipeline will look like this:\n",
    "\n",
    "<img src='penguins/images/session4-pipeline.png' alt='Session 4 Pipeline' width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d585140-7940-4543-9954-b74352e8ff3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "from sagemaker.workflow.parameters import ParameterBoolean\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca780cf-d2f2-4de7-91c5-ce0dcc829ee9",
   "metadata": {},
   "source": [
    "## Step 1 - Deploy Latest Model From Registry\n",
    "\n",
    "Let's get the latest approved model from the Model Registry and deploy it to an endpoint.\n",
    "\n",
    "We can use `boto3` to query the list of approved models and get the latest one. Check the [boto3 SageMaker Client API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) for a list of every available method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7804ee05-c2de-48ac-96b5-dbb2ebe9836f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'penguins',\n",
       " 'ModelPackageVersion': 9,\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:325223348818:model-package/penguins/9',\n",
       " 'CreationTime': datetime.datetime(2023, 6, 6, 13, 26, 57, 673000, tzinfo=tzlocal()),\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelApprovalStatus': 'Approved'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = sagemaker_client.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\",\n",
    "    MaxResults=1,\n",
    ")\n",
    "\n",
    "package = response[\"ModelPackageSummaryList\"][0]\n",
    "package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4ff58-235f-4c86-8134-4cfacc45e459",
   "metadata": {},
   "source": [
    "Let's define the name of the endpoint where we'll deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3149cb25-20cb-4561-a29a-cf04f655cf41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"penguins-endpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c2a8f-b164-4fd3-8e3d-a084cd3219b7",
   "metadata": {},
   "source": [
    "Using the ARN of the model package from the Model Registry, we can deploy the model by creating a [ModelPackage](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.ModelPackage) instance and calling its `deploy()` function. The model information lives in the Model Registry, so we don't need to specify anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bdf25ea5-6c5b-4455-9198-9f0aad06ad51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: 9-2023-06-06-13-33-18-267\n",
      "INFO:sagemaker:Creating endpoint-config with name penguins-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name penguins-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "model_package = ModelPackage(\n",
    "    model_package_arn=package[\"ModelPackageArn\"], \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role, \n",
    ")\n",
    "\n",
    "model_package.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1, \n",
    "    instance_type=\"ml.m5.large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f29a26-4016-435c-8ec3-7639cb90399b",
   "metadata": {},
   "source": [
    "Using a [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor) from the endpoint name, we can test our model.\n",
    "\n",
    "The payload we need to provide the model is in CSV format. Notice how the model expects data that's already transformed. We can't provide the original data from our dataset because the model will not work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fc46e-d807-434f-a980-93b054337b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = Predictor(endpoint_name=endpoint_name)\n",
    "\n",
    "payload = \"\"\"\n",
    "0.6569590202313976, -1.0813829646495108, 1.2097102831892812, 0.9226343641317372, 1.0, 0.0, 0.0\n",
    "-0.7751048801481084, 0.8822689351285553,  -1.2168066120762704, 0.9226343641317372, 0.0, 1.0, 0.0\n",
    "-0.837387834894918, 0.3386660813829646, -0.26237731892812, -1.92351941317372, 0.0, 0.0, 1.0\n",
    "\"\"\"\n",
    "\n",
    "response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "\n",
    "# We can decode the output of the endpoint and print the \"predictions\" key.\n",
    "predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"]\n",
    "print(f\"Prediction: {np.argmax(predictions, axis=1)}\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75271c1c-b661-4569-aeb6-93f2c6621bdb",
   "metadata": {},
   "source": [
    "Let's now delete the endpoint.predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ce0a7-ca8f-46c0-bcdf-2347b0d2f1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc27c6-31d4-454d-ae5b-1aeba25f0ac0",
   "metadata": {},
   "source": [
    "## Step 2 - Preparing the Inference Code\n",
    "\n",
    "Deploying the model we trained directly to an endpoint doesn't lets us control the data that goes in and comes out of the endpoint. Fortunately, SageMaker allows us to include an `inference.py` file with the model assets from where we can control how the endpoint works. You can see more information about how this works by checking the [SageMaker TensorFlow Serving Container](https://github.com/aws/sagemaker-tensorflow-serving-container) documentation.\n",
    "\n",
    "We want our endpoint to handle unprocessed data in JSON format. Here is an example of the payload we want the endpoint to support:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"island\": \"Biscoe\",\n",
    "    \"culmen_length_mm\": 48.6,\n",
    "    \"culmen_depth_mm\": 16.0,\n",
    "    \"flipper_length_mm\": 230.0,\n",
    "    \"body_mass_g\": 5800.0,\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Let's start by setting up a local folder where we will create the `inference.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ac9cec59-c812-4ca9-9f71-d6725de03c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CODE_FOLDER = PENGUINS_FOLDER / \"code\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34c1c1-66af-4cf2-b103-8612bd60ce0e",
   "metadata": {},
   "source": [
    "We will include the inference code as part of the model assets to control the inference process on the SageMaker endpoint. SageMaker will automatically call the `handler()` function for every request to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "57e7ad9e-598b-4e28-9a20-a93ad2bb5f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $CODE_FOLDER/inference.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pickle import load\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PIPELINE_FILE = Path(\"/tmp\") / \"pipeline.pkl\"\n",
    "CLASSES_FILE = Path(\"/tmp\") / \"classes.csv\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def handler(data, context):\n",
    "    \"\"\"\n",
    "    This is the entrypoint that will be called by SageMaker when the endpoint\n",
    "    receives a request. You can see more information at \n",
    "    https://github.com/aws/sagemaker-tensorflow-serving-container.\n",
    "    \"\"\"\n",
    "    print(\"Handling endpoint request\")\n",
    "    \n",
    "    instance = _process_input(data, context)\n",
    "    output = _predict(instance, context)\n",
    "    return _process_output(output, context)\n",
    "\n",
    "\n",
    "def _process_input(data, context):\n",
    "    print(\"Processing input data...\")\n",
    "    \n",
    "    if context is None:\n",
    "        # The context will be None when we are testing the code\n",
    "        # directly from a notebook. In that case, we can use the\n",
    "        # data directly.\n",
    "        endpoint_input = data\n",
    "    elif context.request_content_type in (\"application/json\", \"application/octet-stream\"):\n",
    "        # When the endpoint is running, we will receive a context\n",
    "        # object. We need to parse the input and turn it into \n",
    "        # JSON in that case.\n",
    "        endpoint_input = json.loads(data.read().decode(\"utf-8\"))\n",
    "\n",
    "        if endpoint_input is None:\n",
    "            raise ValueError(\"There was an error parsing the input request.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {context.request_content_type or 'unknown'}\")\n",
    "        \n",
    "    return _transform(endpoint_input)\n",
    "\n",
    "\n",
    "def _predict(instance, context):\n",
    "    print(\"Sending input data to model to make a prediction...\")\n",
    "    \n",
    "    model_input = json.dumps({\"instances\": [instance]})\n",
    "    \n",
    "    if context is None:\n",
    "        # The context will be None when we are testing the code\n",
    "        # directly from a notebook. In that case, we want to return\n",
    "        # a fake prediction back.\n",
    "        result = {\n",
    "            \"predictions\": [\n",
    "                [0.2, 0.5, 0.3]\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        # When the endpoint is running, we will receive a context\n",
    "        # object. In that case we need to send the instance to the\n",
    "        # model to get a prediction back.\n",
    "        response = requests.post(context.rest_uri, data=model_input)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(response.content.decode('utf-8'))\n",
    "            \n",
    "        result = json.loads(response.content)\n",
    "    \n",
    "    print(f\"Response: {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def _process_output(output, context):\n",
    "    print(\"Processing prediction received from the model...\")\n",
    "    \n",
    "    response_content_type = \"application/json\" if context is None else context.accept_header\n",
    "    \n",
    "    prediction = np.argmax(output[\"predictions\"][0])\n",
    "    confidence = output[\"predictions\"][0][prediction]\n",
    "    \n",
    "    print(f\"Prediction: {prediction}. Confidence: {confidence}\")\n",
    "    \n",
    "    result = json.dumps({\n",
    "        \"prediction\": _get_class(prediction),\n",
    "        \"confidence\": confidence\n",
    "    }), response_content_type\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _get_pipeline():\n",
    "    \"\"\"\n",
    "    This function will download the Scikit-Learn pipeline from S3 if it doesn't\n",
    "    already exist. The function will use the `S3_LOCATION` environment\n",
    "    variable to determine the location of the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not PIPELINE_FILE.exists():\n",
    "        s3_uri = os.environ.get(\"S3_LOCATION\", None)\n",
    "        \n",
    "        s3_parts = s3_uri.split('/', 3)\n",
    "        bucket = s3_parts[2]\n",
    "        key = s3_parts[3]\n",
    "\n",
    "        s3.Bucket(bucket).download_file(f\"{key}/pipeline.pkl\", str(PIPELINE_FILE))\n",
    "        \n",
    "    return load(open(PIPELINE_FILE, 'rb'))\n",
    "\n",
    "\n",
    "def _get_class(prediction):\n",
    "    \"\"\"\n",
    "    This function returns the class name of a given prediction. \n",
    "    \n",
    "    The function downloads the file with the list of classes from S3 if it doesn't\n",
    "    already exist. The function will use the `S3_LOCATION` environment\n",
    "    variable to determine the location of the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not CLASSES_FILE.exists():\n",
    "        s3_uri = os.environ.get(\"S3_LOCATION\", None)\n",
    "        \n",
    "        s3_parts = s3_uri.split('/', 3)\n",
    "        bucket = s3_parts[2]\n",
    "        key = s3_parts[3]\n",
    "\n",
    "        s3.Bucket(bucket).download_file(f\"{key}/classes.csv\", str(CLASSES_FILE))\n",
    "            \n",
    "    classes = np.loadtxt(CLASSES_FILE, delimiter=\",\", dtype=str)\n",
    "    return classes[prediction]\n",
    "\n",
    "\n",
    "def _transform(payload):\n",
    "    \"\"\"\n",
    "    This function transforms the payload in the request using the\n",
    "    Scikit-Learn pipeline that we created during the preprocessing step.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Transforming input data...\")\n",
    "\n",
    "    island = payload.get(\"island\", \"\")\n",
    "    culmen_length_mm = payload.get(\"culmen_length_mm\", 0)\n",
    "    culmen_depth_mm = payload.get(\"culmen_depth_mm\", 0)\n",
    "    flipper_length_mm = payload.get(\"flipper_length_mm\", 0)\n",
    "    body_mass_g = payload.get(\"body_mass_g\", 0)\n",
    "    \n",
    "    data = pd.DataFrame(\n",
    "        columns=[\"island\", \"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"], \n",
    "        data=[[\n",
    "            island, \n",
    "            culmen_length_mm, \n",
    "            culmen_depth_mm, \n",
    "            flipper_length_mm, \n",
    "            body_mass_g\n",
    "        ]]\n",
    "    )\n",
    "    \n",
    "    result = _get_pipeline().transform(data)\n",
    "    return result[0].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e2c6d-977e-43ec-98d4-ec66781af582",
   "metadata": {},
   "source": [
    "SageMaker's default TensorFlow inference container doesn't come with Scikit-Learn installed, so we need to provide a `requirements.txt` file with the libraries we want SageMaker to install in our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ed40d098-d553-4d56-b2eb-f80cec420ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $CODE_FOLDER/requirements.txt\n",
    "\n",
    "numpy==1.19.5\n",
    "pandas==1.2.5\n",
    "scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7e63-39a8-4859-af1e-3e7b1573f9a2",
   "metadata": {},
   "source": [
    "## Step 3 - Testing the Inference Code\n",
    "\n",
    "Let's test the inference code locally to ensure it works before deploying it. The `handler()` function is the entry point that will be called by SageMaker whenever the endpoint receives a request.\n",
    "\n",
    "When testing the inference code, we want to set the `context` to `None` so the function recognizes we are calling it locally. We also want to set the `S3_LOCATION` environment variable to the S3 location of the Scikit-Learn pipeline and the list of supported classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2f56ac31-f733-4a6c-b636-72b20e2d1d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: S3_LOCATION=s3://mlschool/penguins/preprocessing\n"
     ]
    }
   ],
   "source": [
    "%env S3_LOCATION=$preprocessor_destination.default_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a7aca49b-2080-4068-80e6-8b3f10e54177",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling endpoint request\n",
      "Processing input data...\n",
      "Transforming input data...\n",
      "Sending input data to model to make a prediction...\n",
      "Response: {'predictions': [[0.2, 0.5, 0.3]]}\n",
      "Processing prediction received from the model...\n",
      "Prediction: 1. Confidence: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator SimpleImputer from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator StandardScaler from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator Pipeline from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator ColumnTransformer from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('{\"prediction\": \"\\'Chinstrap\\'\", \"confidence\": 0.5}', 'application/json')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from penguins.code.inference import handler\n",
    "\n",
    "handler(\n",
    "    data={\n",
    "        \"island\": \"Biscoe\",\n",
    "        \"culmen_length_mm\": 48.6,\n",
    "        \"culmen_depth_mm\": 16.0,\n",
    "        \"flipper_length_mm\": 230.0,\n",
    "        \"body_mass_g\": 5800.0,\n",
    "    }, \n",
    "    context=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63f158-06f3-438b-b489-bb0496f2eddd",
   "metadata": {},
   "source": [
    "## Step 4 - Registering the Model\n",
    "\n",
    "We can now register a new [TensorFlowModel](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-serving-model). We must also ensure SageMaker repackages the model assets to include the `inference.py` file.\n",
    "\n",
    "SageMaker triggers a repack whenever we specify the `source_dir` attribute. We want that attribute to point to the local folder containing the `inference.py` file. SageMaker will automatically modify the original `model.tar.gz` package to include a `/code` folder containing the file. Since we need access to Scikit-Learn in our script, we can include a `requirements.txt` file in the same `/code` folder, and SageMaker will install everything in it. To repack the model assets, SageMaker will automatically include a new step in the pipeline right before registering the model.\n",
    "\n",
    "Here is what the new `model.tar.gz` package will look like:\n",
    "\n",
    "```\n",
    "model/\n",
    "    |--[model_version_number]\n",
    "        |--assets/\n",
    "        |--variables/\n",
    "        |--saved_model.pb\n",
    "code/\n",
    "    |--inference.py\n",
    "    |--requirements.txt\n",
    "```\n",
    "\n",
    "Let's use a [ModelStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.model_step.ModelStep) to register the model. Notice the following:\n",
    "\n",
    "* `model_data`: We use the model assets we generated during the Training or Tuning Step. We determined which assets to use back in Session 4 and stored them in the `model_data` variable.\n",
    "* `source_dir`: This points to the local folder containing the `inference.py` file. SageMaker will trigger a repack to include the `/code` folder in the model assets.\n",
    "* `env`: Our custom inference code expects an environment variable `PIPELINE_S3_LOCATION` to point to the location of the Scikit-Learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "07513884-c7bd-4710-9730-f8ca7fe904a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.tensorflow.model:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "/usr/local/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py:270: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = TensorFlowModel(\n",
    "    model_data=train_model_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=str(CODE_FOLDER),\n",
    "    env={\n",
    "        \"PIPELINE_S3_LOCATION\": preprocessor_destination,\n",
    "    },\n",
    "    framework_version=\"2.6\",\n",
    "    sagemaker_session=PipelineSession(),\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "register_model_step = ModelStep(\n",
    "    name=\"register-model\",\n",
    "    step_args=model.register(\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        model_metrics=model_metrics,\n",
    "        approval_status=\"Approved\",\n",
    "        \n",
    "        content_types=[\"application/json\"],\n",
    "        response_types=[\"application/json\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"TENSORFLOW\",\n",
    "        framework_version=\"2.6\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b60d5-6be3-428f-9979-4fe9eebfc5eb",
   "metadata": {},
   "source": [
    "## Step 5 - Deploying the Model\n",
    "\n",
    "Let's use a [Lambda Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-lambda) to deploy the model automatically.\n",
    "\n",
    "Let's start by writing the Lambda function to take the model information and create a new hosting endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "28c75afd-d0d0-47f4-b7b6-9d590c5b600e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/lambda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $PENGUINS_FOLDER/lambda.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "sagemaker = boto3.client(\"sagemaker\")\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    \n",
    "    data_capture_percentage = event[\"data_capture_percentage\"]\n",
    "    data_capture_destination = event[\"data_capture_destination\"]\n",
    "    \n",
    "    role = event[\"role\"]\n",
    "    \n",
    "    timestamp = time.strftime(\"%m%d%H%M%S\", time.localtime())\n",
    "    model_name = f\"penguins-model-{timestamp}\"\n",
    "    endpoint_config_name = f\"penguins-endpoint-config-{timestamp}\"\n",
    "\n",
    "    sagemaker.create_model(\n",
    "        ModelName=model_name, \n",
    "        ExecutionRoleArn=role, \n",
    "        Containers=[{\n",
    "            \"ModelPackageName\": model_package_arn\n",
    "        }] \n",
    "    )\n",
    "\n",
    "    sagemaker.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"ModelName\": model_name,\n",
    "                \"InstanceType\": \"ml.m5.large\",\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "        DataCaptureConfig={\n",
    "            \"EnableCapture\": True,\n",
    "            \"InitialSamplingPercentage\": data_capture_percentage,\n",
    "            \"DestinationS3Uri\": data_capture_destination,\n",
    "            \"CaptureOptions\": [\n",
    "                {\n",
    "                    'CaptureMode': \"Input\"\n",
    "                },\n",
    "                {\n",
    "                    'CaptureMode': \"Output\"\n",
    "                },\n",
    "            ],\n",
    "            \"CaptureContentTypeHeader\": {\n",
    "                \"JsonContentTypes\": [\n",
    "                    \"application/json\",\n",
    "                    \"application/octect-stream\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    sagemaker.create_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\"Endpoint deployed successfully\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a4446-6db4-4515-bb1d-88ee21013bb2",
   "metadata": {},
   "source": [
    "We need to ensure our Lambda function has permission to interact with SageMaker, so let's create a new role to run the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f9941979-c085-4ef9-8c66-908378d89a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lambda_role(role_name):\n",
    "    try:\n",
    "        response = iam_client.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description=\"Lambda Pipeline Role\"\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "\n",
    "        iam_client.attach_role_policy(\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "            RoleName=role_name\n",
    "        )\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        response = iam_client.get_role(RoleName=role_name)\n",
    "        return response['Role']['Arn']\n",
    "\n",
    "\n",
    "lambda_role = create_lambda_role(\"lambda-pipeline-role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4dcc96-2890-43ff-a6cd-f2a9969adc52",
   "metadata": {},
   "source": [
    "## Step 6 - Setting up the Lambda Step\n",
    "\n",
    "Let's define the [LambdaStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.lambda_step.LambdaStep) that will run the function to deploy the model.\n",
    "\n",
    "We can use [Data Capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html) to record the inputs and outputs of the endpoint to use them later for monitoring the model. We'll enable Data Capture using the following settings:\n",
    "\n",
    "* `data_capture_percentage`: Represents the percentage of information that flows through the endpoint that we want to capture. For this example, we'll set that to 100%.\n",
    "* `data_capture_destination`: Specifies the S3 location where we want to store the captured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7c6eda89-70f3-49b4-8983-90f4db72fd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_capture_percentage = ParameterInteger(\n",
    "    name=\"data_capture_percentage\",\n",
    "    default_value=100,\n",
    ")\n",
    "\n",
    "data_capture_destination = ParameterString(\n",
    "    name=\"data_capture_destination\",\n",
    "    default_value=f\"{S3_FILEPATH}/monitoring/data-capture\",\n",
    ")\n",
    "\n",
    "deploy_fn = Lambda(\n",
    "    function_name=\"deploy_fn\",\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=str(PENGUINS_FOLDER / \"lambda.py\"),\n",
    "    handler=\"lambda.lambda_handler\",\n",
    "    timeout=600\n",
    ")\n",
    "\n",
    "deploy_fn.upsert()\n",
    "\n",
    "deploy_step = LambdaStep(\n",
    "    name=\"deploy\",\n",
    "    lambda_func=deploy_fn,\n",
    "    inputs={\n",
    "        # We use the ARN of the model we registered to\n",
    "        # deploy it to the endpoint.\n",
    "        \"model_package_arn\": register_model_step.properties.ModelPackageArn,\n",
    "\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \n",
    "        \"data_capture_percentage\": data_capture_percentage,\n",
    "        \"data_capture_destination\": data_capture_destination,\n",
    "        \n",
    "        \"role\": role,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe2299-be2a-46a7-809e-6174d44abddb",
   "metadata": {},
   "source": [
    "## Step 7 - Modifying the Condition Step\n",
    "\n",
    "We need to modify the [Condition Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition) to include the new Deploy Step we just created. If the condition succeeds, we will register and deploy the custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "caacdde0-a65f-4225-8303-2b0106da1ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition_step = ConditionStep(\n",
    "    name=\"check-model-accuracy\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[\n",
    "        register_model_step, deploy_step\n",
    "    ],\n",
    "    else_steps=[fail_step], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6d67c-6906-434d-9c41-17a4efeb38d2",
   "metadata": {},
   "source": [
    "## Step 8 - Running the Pipeline\n",
    "\n",
    "We can now run the pipeline. If the pipeline succeeds, there will be a new running endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d1969a5e-2ebf-474f-a275-ae0946c2fab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session4_pipeline = Pipeline(\n",
    "    name=\"penguins-session4-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "        train_dataset_baseline_destination,\n",
    "        test_dataset_baseline_destination,\n",
    "        timestamp_signature,\n",
    "        accuracy_threshold,\n",
    "        data_capture_percentage,\n",
    "        data_capture_destination,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "        train_model_step, \n",
    "        evaluate_model_step,\n",
    "        condition_step\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de309b9-6011-40fe-b03d-ff991700c517",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9e01390f-31a5-44b0-a9ea-2c4697a57aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "session4_pipeline.upsert(role_arn=role)\n",
    "execution = session4_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ca1b1-fdc6-4260-b6f7-ee3c6aab4e76",
   "metadata": {},
   "source": [
    "## Step 9 - Testing the Endpoint\n",
    "\n",
    "We can now create a [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) to test the endpoint with a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e32865-705d-4a2e-a7df-7df198aa8c96",
   "metadata": {},
   "source": [
    "First, let's wait for the endpoint to be ready to service traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "17098e7e-fc61-4027-b98f-7f192ffdf9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waiter = sagemaker_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={\n",
    "        \"Delay\": 10,\n",
    "        \"MaxAttempts\": 30\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016bcfc1-32ba-4ec8-93f6-0c04b00cc4f8",
   "metadata": {},
   "source": [
    "Now that the endpoint is in service, we can create a [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) using a JSON serializer and a deserializer to have it automatically serialize and deserialize the information to and from the endpoint. Check [Serializers](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html) and [Deserializers](https://sagemaker.readthedocs.io/en/stable/api/inference/deserializers.html) for a list of supported serializers and deserializers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "862118a0-c00b-4f5a-9374-68a94dbfbc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b98a7-2f36-4e4e-a277-c5172e5fbe51",
   "metadata": {
    "tags": []
   },
   "source": [
    "Running one example through the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8148ef38-92ed-4f9a-8024-69e2d9d6b5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': '1', 'confidence': 0.349001497}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "    \"island\": \"Dream\",\n",
    "    \"culmen_length_mm\": 46.4,\n",
    "    \"culmen_depth_mm\": 18.6,\n",
    "    \"flipper_length_mm\": 190.0,\n",
    "    \"body_mass_g\": 3450.0,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075ef4c-36a8-4c5d-88ef-2a3e49abdd51",
   "metadata": {},
   "source": [
    "Running another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2476337f-a39b-45d5-ab57-63567c3f0438",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': '2', 'confidence': 0.97760582}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "    \"island\": \"Biscoe\",\n",
    "    \"culmen_length_mm\": 48.6,\n",
    "    \"culmen_depth_mm\": 16.0,\n",
    "    \"flipper_length_mm\": 230.0,\n",
    "    \"body_mass_g\": 5800.0,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fcdcd-cc9e-4cf5-8da1-3dc73cb6b59a",
   "metadata": {},
   "source": [
    "## Step 10 - Cleaning up\n",
    "\n",
    "Before you finish, don't forget to clean up after yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0cf877e9-e1bc-40f0-acaa-64b6a5e6650f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: penguins-endpoint-config-0606134404\n",
      "INFO:sagemaker:Deleting endpoint with name: penguins-endpoint\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544ae36-00b3-4bde-b133-c3a59bb7f1d8",
   "metadata": {},
   "source": [
    "# Session 5 - Data Monitoring\n",
    "\n",
    "This session aims to set up a monitoring process to analyze the quality of the data our endpoint receives in production. For this, we will have SageMaker capture and evaluate the data observed by the endpoint.\n",
    "\n",
    "To enable this functionality, we need a couple of steps:\n",
    "\n",
    "1. Create a baseline to compare the real-time traffic.\n",
    "2. Set up a schedule to continuously evaluate and compare against the baseline.\n",
    "\n",
    "Notice that the Data Quality process uses the baseline dataset we generated during preprocessing. This baseline dataset is the same unprocessed train set in JSON format. We do this because we transformed the train data during the preprocessing step, but we need raw data because that's what the endpoint expects.\n",
    "\n",
    "Check [Amazon SageMaker Model Monitor](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_monitoring.html) for a brief explanation of how to use SageMaker's Model Monitoring functionality. [Monitor models for data and model quality, bias, and explainability](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html) is a much more extensive guide to monitoring in Amazon SageMaker.\n",
    "\n",
    "Here is what the Pipeline will look like at the end of this session:\n",
    "\n",
    "<img src='penguins/images/session5-pipeline.png' alt='Session 5 Pipeline' width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2c7e2f9d-cc75-46bc-8700-f7123292fac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from threading import Thread, Event\n",
    "\n",
    "from IPython.display import JSON\n",
    "\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.quality_check_step import DataQualityCheckConfig, QualityCheckStep\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "from sagemaker.workflow.parameters import ParameterBoolean\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import CronExpressionGenerator, EndpointInput, DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c26ac4-5d30-41e9-8952-e4deb39de819",
   "metadata": {},
   "source": [
    "## Step 1 - Generating a Baseline\n",
    "\n",
    "Let's now configure the [Quality Check Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-quality-check) and feed it the train set we generated in the preprocessing step.\n",
    "\n",
    "We can configure the instance that will run the quality check using the [CheckJobConfig](https://sagemaker.readthedocs.io/en/v2.73.0/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.check_job_config.CheckJobConfig) class, and we can use the `DataQualityCheckConfig` class to configure the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0b80bcab-d2c5-437c-a1c8-8eea208c0e29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "data_quality_location = f\"{S3_FILEPATH}/monitoring/data-quality/\"\n",
    "\n",
    "data_quality_baseline_step = QualityCheckStep(\n",
    "    name=\"generate-data-quality-baseline\",\n",
    "    \n",
    "    check_job_config = CheckJobConfig(\n",
    "        instance_type=\"ml.t3.xlarge\",\n",
    "        instance_count=1,\n",
    "        volume_size_in_gb=20,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    ),\n",
    "    \n",
    "    quality_check_config = DataQualityCheckConfig(\n",
    "        # We will use the train dataset we generated during the preprocessing \n",
    "        # step to generate the data quality baseline.\n",
    "        baseline_dataset=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\"train-baseline\"].S3Output.S3Uri,\n",
    "\n",
    "        dataset_format=DatasetFormat.json(lines=True),\n",
    "        # output_s3_uri=Join(on='/', values=[S3_FILEPATH, \"monitoring\", \"data-quality\"]),\n",
    "        output_s3_uri=data_quality_location\n",
    "    ),\n",
    "    \n",
    "    skip_check=True,\n",
    "    register_new_baseline=True,\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08936035-1a36-4a0c-9529-76cc60b7850d",
   "metadata": {},
   "source": [
    "## Step 2 - Running the Pipeline\n",
    "\n",
    "We can now run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8c926da4-043a-4237-ac71-90ef18cf806b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session5_pipeline = Pipeline(\n",
    "    name=\"penguins-session5-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "        train_dataset_baseline_destination,\n",
    "        test_dataset_baseline_destination,\n",
    "        timestamp_signature,\n",
    "        data_capture_percentage,\n",
    "        data_capture_destination,       \n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "        data_quality_baseline_step,\n",
    "        train_model_step, \n",
    "        evaluate_model_step,\n",
    "        condition_step\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9aeb7-1c63-470a-80dc-d5932715c088",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0eb61f6e-2e07-4b11-9c50-3df8ab2164bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "session5_pipeline.upsert(role_arn=role)\n",
    "execution = session5_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19defdd9-f9ab-4283-b550-389e67e546c3",
   "metadata": {},
   "source": [
    "## Step 3 - Setting Up a Predictor\n",
    "\n",
    "We can now create a [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) from the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "69bd29cd-d65c-488c-ab0f-27d99469c876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waiter = sagemaker_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={\n",
    "        \"Delay\": 10,\n",
    "        \"MaxAttempts\": 30\n",
    "    }\n",
    ")\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc1535-7854-4901-b9f0-3c3250b8252a",
   "metadata": {},
   "source": [
    "## Step 4 - Generating Endpoint Traffic\n",
    "\n",
    "Let's generate some traffic for our endpoint so we can test the monitoring functionality. We will repeatedly send every sample from the dataset to the endpoint to simulate real prediction requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "480e5feb-915a-4e38-9b96-1f092fa09507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 334 predictions...\n"
     ]
    }
   ],
   "source": [
    "def generate_traffic():\n",
    "    \n",
    "    def _predict(data, predictor, stop_traffic_thread):\n",
    "        for index in data.index:\n",
    "            payload = {\n",
    "                \"island\": data[\"island\"][index],\n",
    "                \"culmen_length_mm\": data[\"culmen_length_mm\"][index],\n",
    "                \"culmen_depth_mm\": data[\"culmen_depth_mm\"][index],\n",
    "                \"flipper_length_mm\": data[\"flipper_length_mm\"][index],\n",
    "                \"body_mass_g\": data[\"body_mass_g\"][index],\n",
    "            }\n",
    "\n",
    "            predictor.predict(payload, inference_id=str(index))\n",
    "            sleep(1)\n",
    "\n",
    "            if stop_traffic_thread.is_set():\n",
    "                break\n",
    "\n",
    "    def _generate_prediction_data(data, predictor, stop_traffic_thread):\n",
    "        while True:\n",
    "            print(f\"Generating {data.shape[0]} predictions...\")\n",
    "            _predict(data, predictor, stop_traffic_thread)\n",
    "            \n",
    "            if stop_traffic_thread.is_set():\n",
    "                break\n",
    "\n",
    "                \n",
    "    stop_traffic_thread = Event()\n",
    "    data = pd.read_csv(LOCAL_FILEPATH).dropna()\n",
    "    \n",
    "    traffic_thread = Thread(\n",
    "        target=_generate_prediction_data,\n",
    "        args=(data, predictor, stop_traffic_thread,)\n",
    "    )\n",
    "    \n",
    "    traffic_thread.start()\n",
    "    \n",
    "    return stop_traffic_thread, traffic_thread\n",
    "\n",
    "\n",
    "stop_traffic_thread, traffic_thread = generate_traffic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a24f8-9fac-47ef-b707-4919d2c14ecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5 - Checking the Captured Data\n",
    "\n",
    "Let's check the S3 location where the endpoint stores the requests and responses that it receives.\n",
    "\n",
    "Notice that it make take a few minutes for the first few files to show up in S3. Keep running the following line until you get some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3ba479ee-aefc-4480-9ebf-cd3e679edbe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2023/04/28/19/16-06-992-abf52eaa-40cd-4fb9-916a-96bfe20252c0.jsonl',\n",
       " 's3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2023/04/28/19/17-07-857-a731227c-df7d-425c-8df5-89206a5d8875.jsonl',\n",
       " 's3://mlschool/penguins/monitoring/data-capture/penguins-endpoint/AllTraffic/2023/04/28/19/18-08-423-d0343cfa-b491-4099-a816-ebe4f3a149dc.jsonl']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = S3Downloader.list(data_capture_destination.default_value)[:3]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52455abe-3eea-46e5-a4b3-95a83ba5f926",
   "metadata": {},
   "source": [
    "These files contain the data captured by the endpoint in a SageMaker-specific JSON-line format. Each inference request is captured in a single line in the `jsonl` file. The line contains both the input and output merged together.\n",
    "\n",
    "Let's read the first line from the first file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ea31c263-f941-4177-bdc7-842e4b6172b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"application/json\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"{\\\"island\\\": \\\"Dream\\\", \\\"culmen_length_mm\\\": 46.4, \\\"culmen_depth_mm\\\": 18.6, \\\"flipper_length_mm\\\": 190.0, \\\"body_mass_g\\\": 3450.0}\",\n",
      "      \"encoding\": \"JSON\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"application/json\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"{\\\"prediction\\\": \\\"0\\\", \\\"confidence\\\": 0.497521222}\",\n",
      "      \"encoding\": \"JSON\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"33532b83-7f7c-4335-9c8a-a8023c3799c6\",\n",
      "    \"inferenceTime\": \"2023-04-28T19:16:06Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if len(files):\n",
    "    lines = S3Downloader.read_file(files[0])\n",
    "    print(json.dumps(json.loads(lines.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60810a83-edba-464b-8b41-d96a89991769",
   "metadata": {},
   "source": [
    "## Step 6 - Statistics and Constraints\n",
    "\n",
    "Our pipeline generated baseline statistics and constraints using our train set. We can take a look at what these values look like by downloading them from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4c6670fa-8560-4c12-9400-235bf1b10a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "dataset": {
        "item_count": 238
       },
       "features": [
        {
         "inferred_type": "Fractional",
         "name": "body_mass_g",
         "numerical_statistics": {
          "common": {
           "num_missing": 0,
           "num_present": 238
          },
          "distribution": {
           "kll": {
            "buckets": [
             {
              "count": 15,
              "lower_bound": 2850,
              "upper_bound": 3195
             },
             {
              "count": 40,
              "lower_bound": 3195,
              "upper_bound": 3540
             },
             {
              "count": 45,
              "lower_bound": 3540,
              "upper_bound": 3885
             },
             {
              "count": 31,
              "lower_bound": 3885,
              "upper_bound": 4230
             },
             {
              "count": 26,
              "lower_bound": 4230,
              "upper_bound": 4575
             },
             {
              "count": 32,
              "lower_bound": 4575,
              "upper_bound": 4920
             },
             {
              "count": 18,
              "lower_bound": 4920,
              "upper_bound": 5265
             },
             {
              "count": 17,
              "lower_bound": 5265,
              "upper_bound": 5610
             },
             {
              "count": 12,
              "lower_bound": 5610,
              "upper_bound": 5955
             },
             {
              "count": 2,
              "lower_bound": 5955,
              "upper_bound": 6300
             }
            ],
            "sketch": {
             "data": [
              [
               3700,
               4100,
               3525,
               4450,
               3900,
               2850,
               3575,
               4900,
               3325,
               2925,
               4925,
               3900,
               3325,
               3250,
               4700,
               3950,
               3900,
               4400,
               5850,
               4750,
               3950,
               3400,
               3700,
               4100,
               3650,
               6000,
               3500,
               4200,
               3800,
               3300,
               3650,
               4600,
               4700,
               4200,
               5000,
               5500,
               4450,
               4100,
               5500,
               5300,
               3450,
               4150,
               3000,
               4875,
               3450,
               4050,
               3700,
               3050,
               4600,
               3700,
               3500,
               5000,
               3800,
               3675,
               3800,
               3200,
               4250,
               3800,
               4400,
               3900,
               3775,
               3550,
               3500,
               3300,
               3400,
               4750,
               5050,
               5300,
               3800,
               3625,
               4875,
               3750,
               5400,
               4300,
               3400,
               5650,
               4875,
               4500,
               4925,
               4050,
               5000,
               4200,
               3450,
               3725,
               5000,
               3550,
               3700,
               4400,
               3950,
               3775,
               4350,
               3700,
               5550,
               4725,
               5100,
               5200,
               4800,
               4600,
               5650,
               4150,
               3450,
               5700,
               5100,
               5850,
               3900,
               5000,
               5000,
               3050,
               5050,
               3950,
               4075,
               4600,
               3800,
               4000,
               3075,
               3300,
               5250,
               3450,
               5350,
               4950,
               4250,
               4375,
               4775,
               3700,
               3800,
               3550,
               3400,
               3350,
               3250,
               5550,
               3800,
               5400,
               3425,
               5550,
               4600,
               4400,
               3750,
               4475,
               4675,
               3600,
               3700,
               3675,
               4250,
               3350,
               4400,
               4650,
               4750,
               4700,
               3800,
               3525,
               3850,
               3050,
               3600,
               3425,
               3500,
               4725,
               4650,
               3325,
               3600,
               4725,
               3500,
               5400,
               3300,
               4500,
               3150,
               3400,
               4100,
               3150,
               4150,
               3300,
               4300,
               3600,
               4850,
               6300,
               4100,
               3750,
               3175,
               4450,
               4700,
               4300,
               3450,
               3150,
               5400,
               5800,
               4950,
               3550,
               5350,
               5550,
               4300,
               3700,
               3650,
               3500,
               3200,
               4150,
               3775,
               3875,
               4850,
               5300,
               3200,
               3725,
               2850,
               3950,
               4450,
               4275,
               3775,
               3550,
               4625,
               3450,
               3275,
               5700,
               3000,
               5200,
               5750,
               4200,
               5800,
               4250,
               3350,
               4625,
               3175,
               4575,
               5700,
               3950,
               4850,
               4800,
               5850,
               5500,
               4450,
               4200,
               5950,
               4150,
               3150,
               3900,
               4350,
               4975,
               3325,
               4750,
               5300,
               4300
              ]
             ],
             "parameters": {
              "c": 0.64,
              "k": 2048
             }
            }
           }
          },
          "max": 6300,
          "mean": 4212.18487394958,
          "min": 2850,
          "std_dev": 781.350140111253,
          "sum": 1002500
         }
        },
        {
         "inferred_type": "Fractional",
         "name": "culmen_depth_mm",
         "numerical_statistics": {
          "common": {
           "num_missing": 0,
           "num_present": 238
          },
          "distribution": {
           "kll": {
            "buckets": [
             {
              "count": 14,
              "lower_bound": 13.1,
              "upper_bound": 13.94
             },
             {
              "count": 28,
              "lower_bound": 13.94,
              "upper_bound": 14.78
             },
             {
              "count": 22,
              "lower_bound": 14.78,
              "upper_bound": 15.620000000000001
             },
             {
              "count": 26,
              "lower_bound": 15.620000000000001,
              "upper_bound": 16.46
             },
             {
              "count": 23,
              "lower_bound": 16.46,
              "upper_bound": 17.3
             },
             {
              "count": 38,
              "lower_bound": 17.3,
              "upper_bound": 18.14
             },
             {
              "count": 41,
              "lower_bound": 18.14,
              "upper_bound": 18.98
             },
             {
              "count": 26,
              "lower_bound": 18.98,
              "upper_bound": 19.82
             },
             {
              "count": 11,
              "lower_bound": 19.82,
              "upper_bound": 20.66
             },
             {
              "count": 9,
              "lower_bound": 20.66,
              "upper_bound": 21.5
             }
            ],
            "sketch": {
             "data": [
              [
               16.8,
               18.8,
               19.4,
               18.5,
               18.9,
               16.6,
               17.1,
               14.4,
               16.4,
               18.6,
               13.7,
               19.4,
               18.4,
               18.7,
               15,
               20,
               18.4,
               14.4,
               16,
               13.8,
               19.5,
               18.4,
               19.9,
               14.3,
               17,
               16.3,
               17,
               14.6,
               17.8,
               17.8,
               18.7,
               14.3,
               14.2,
               13.8,
               15.2,
               15,
               18.1,
               18.5,
               16.1,
               15.8,
               17.9,
               19,
               16.9,
               14,
               19,
               18.1,
               17.3,
               17.1,
               18.8,
               17.8,
               17.9,
               15.6,
               21.2,
               17.9,
               19.4,
               17.6,
               17.9,
               18.9,
               13.9,
               20,
               20.5,
               17.2,
               19.2,
               17.9,
               18.1,
               14.5,
               14.5,
               14.9,
               17.4,
               17.8,
               14.6,
               18.2,
               16.3,
               19.6,
               17.5,
               15.7,
               15.7,
               20.7,
               15.8,
               19.6,
               15,
               19.4,
               18.8,
               17.9,
               14.5,
               20.3,
               17.3,
               21.1,
               17.8,
               20.3,
               13.6,
               17.8,
               17,
               14.6,
               13.3,
               14.8,
               20.7,
               20.3,
               15,
               21.2,
               17,
               16,
               15.1,
               14.6,
               20.7,
               13.1,
               15,
               15.5,
               15.4,
               18.9,
               18.3,
               14.2,
               19.5,
               18.5,
               16,
               20,
               17.3,
               17.9,
               15.1,
               13.7,
               20.2,
               14.1,
               19,
               17.8,
               18.8,
               18.6,
               17.1,
               17.8,
               16.6,
               15.9,
               18.8,
               16.1,
               17.6,
               15.3,
               14.4,
               13.4,
               19.1,
               18.5,
               19.6,
               18.7,
               17.1,
               17.3,
               18.5,
               16.7,
               18.2,
               14.4,
               15,
               17.6,
               18.8,
               18.7,
               18.3,
               15.9,
               17.3,
               19,
               19.8,
               20,
               13.7,
               18.6,
               18.8,
               13.8,
               17.7,
               15.9,
               19.3,
               13.2,
               17.2,
               16.5,
               19,
               18,
               13.5,
               17.3,
               18.9,
               17.3,
               15,
               15.2,
               19.1,
               18.7,
               17,
               19.4,
               14.2,
               20.8,
               18.6,
               18.6,
               17.1,
               16.2,
               15,
               17.5,
               15.7,
               16.1,
               18.9,
               17.5,
               19.2,
               18.6,
               17.9,
               18.9,
               18.7,
               18.5,
               14.3,
               14.1,
               17.2,
               19.8,
               17.1,
               19.2,
               14.5,
               19.5,
               18.2,
               16.1,
               14.5,
               19.3,
               17.6,
               16.3,
               16.8,
               15.3,
               15.7,
               21.5,
               16,
               20,
               16.2,
               14.4,
               17.5,
               14,
               15.2,
               18,
               14.2,
               14.6,
               15.7,
               15.8,
               14.1,
               13.9,
               16.4,
               21.1,
               16.9,
               19.5,
               18.5,
               15.5,
               17,
               15,
               14.2,
               18.3
              ]
             ],
             "parameters": {
              "c": 0.64,
              "k": 2048
             }
            }
           }
          },
          "max": 21.5,
          "mean": 17.14327731092436,
          "min": 13.1,
          "std_dev": 2.0363753468726613,
          "sum": 4080.099999999998
         }
        },
        {
         "inferred_type": "Fractional",
         "name": "culmen_length_mm",
         "numerical_statistics": {
          "common": {
           "num_missing": 0,
           "num_present": 238
          },
          "distribution": {
           "kll": {
            "buckets": [
             {
              "count": 5,
              "lower_bound": 32.1,
              "upper_bound": 34.69
             },
             {
              "count": 26,
              "lower_bound": 34.69,
              "upper_bound": 37.28
             },
             {
              "count": 37,
              "lower_bound": 37.28,
              "upper_bound": 39.87
             },
             {
              "count": 28,
              "lower_bound": 39.87,
              "upper_bound": 42.46
             },
             {
              "count": 22,
              "lower_bound": 42.46,
              "upper_bound": 45.05
             },
             {
              "count": 53,
              "lower_bound": 45.05,
              "upper_bound": 47.64
             },
             {
              "count": 34,
              "lower_bound": 47.64,
              "upper_bound": 50.230000000000004
             },
             {
              "count": 28,
              "lower_bound": 50.230000000000004,
              "upper_bound": 52.82
             },
             {
              "count": 4,
              "lower_bound": 52.82,
              "upper_bound": 55.41
             },
             {
              "count": 1,
              "lower_bound": 55.41,
              "upper_bound": 58
             }
            ],
            "sketch": {
             "data": [
              [
               40.9,
               51,
               45.6,
               50.8,
               40.9,
               36.5,
               45.9,
               46.5,
               48.1,
               37.9,
               47.2,
               37.2,
               34.4,
               51.5,
               46.4,
               38.8,
               40.8,
               45.1,
               55.1,
               45.2,
               51.9,
               50.5,
               51.3,
               44.5,
               45.7,
               51.1,
               45.5,
               45.8,
               46.6,
               39.5,
               39,
               48.2,
               45.8,
               45.3,
               50.5,
               49.1,
               39.6,
               43.2,
               46.8,
               45.2,
               36,
               52,
               37,
               47.5,
               38.7,
               52,
               37.8,
               39,
               39.6,
               58,
               46.5,
               46.4,
               38.6,
               50.9,
               50.6,
               41.1,
               39.7,
               35.3,
               45.7,
               38.2,
               37.3,
               39.6,
               43.1,
               46.7,
               43.5,
               45.5,
               45.1,
               46.2,
               39.5,
               38.9,
               46.9,
               51.3,
               48.4,
               49,
               48.5,
               54.3,
               44.5,
               42.5,
               49.4,
               50.5,
               45.5,
               35.1,
               52.2,
               35,
               45.1,
               41.3,
               47,
               34.6,
               45.2,
               51.7,
               44,
               36.6,
               52.1,
               47.4,
               44.9,
               45.2,
               52,
               45.6,
               47.8,
               42.3,
               40.2,
               49.6,
               46.1,
               48.4,
               39.6,
               42.9,
               50.1,
               32.1,
               45,
               40.5,
               42.7,
               47.5,
               36.3,
               41.5,
               37.7,
               50.3,
               44.4,
               35,
               48.7,
               42.6,
               42,
               46.2,
               43.2,
               46.4,
               40.6,
               40.6,
               34,
               37.3,
               45.2,
               50.5,
               50.2,
               49.9,
               38.1,
               50.4,
               43.4,
               43.3,
               37.6,
               37.5,
               39.2,
               37.7,
               36,
               49.8,
               42.8,
               42.5,
               49.2,
               46.2,
               49.6,
               42.9,
               36.7,
               45.4,
               47.6,
               35.2,
               45.7,
               41.1,
               37.7,
               41,
               40.9,
               41.1,
               38.9,
               47.3,
               39.6,
               50.5,
               37.6,
               46.1,
               36.2,
               37,
               50.8,
               36.5,
               42,
               36.2,
               40.8,
               42.4,
               48.5,
               49.2,
               41.1,
               39.1,
               38.1,
               41.8,
               42.8,
               54.2,
               46.4,
               37.9,
               52.2,
               49.5,
               47.5,
               39,
               48.7,
               49,
               40.1,
               35.5,
               51.3,
               36.9,
               40.5,
               45.8,
               50.2,
               41.4,
               46.8,
               48.5,
               34.6,
               52.7,
               36.4,
               38.3,
               43.2,
               42.2,
               49.6,
               36.2,
               49.1,
               36.7,
               38.8,
               50,
               37.3,
               46.7,
               50.4,
               46,
               48.6,
               37.8,
               35.5,
               48.4,
               35.6,
               43.3,
               50,
               41.6,
               46.6,
               45.4,
               49.3,
               53.4,
               48.7,
               45.5,
               45.2,
               39.2,
               35.7,
               50,
               40.3,
               47.2,
               36.4,
               47.7,
               51.3,
               41.5
              ]
             ],
             "parameters": {
              "c": 0.64,
              "k": 2048
             }
            }
           }
          },
          "max": 58,
          "mean": 43.97058823529412,
          "min": 32.1,
          "std_dev": 5.34084236174134,
          "sum": 10465.000000000002
         }
        },
        {
         "inferred_type": "Fractional",
         "name": "flipper_length_mm",
         "numerical_statistics": {
          "common": {
           "num_missing": 0,
           "num_present": 238
          },
          "distribution": {
           "kll": {
            "buckets": [
             {
              "count": 2,
              "lower_bound": 172,
              "upper_bound": 177.9
             },
             {
              "count": 15,
              "lower_bound": 177.9,
              "upper_bound": 183.8
             },
             {
              "count": 35,
              "lower_bound": 183.8,
              "upper_bound": 189.7
             },
             {
              "count": 55,
              "lower_bound": 189.7,
              "upper_bound": 195.6
             },
             {
              "count": 34,
              "lower_bound": 195.6,
              "upper_bound": 201.5
             },
             {
              "count": 8,
              "lower_bound": 201.5,
              "upper_bound": 207.4
             },
             {
              "count": 30,
              "lower_bound": 207.4,
              "upper_bound": 213.3
             },
             {
              "count": 34,
              "lower_bound": 213.3,
              "upper_bound": 219.2
             },
             {
              "count": 17,
              "lower_bound": 219.2,
              "upper_bound": 225.1
             },
             {
              "count": 8,
              "lower_bound": 225.1,
              "upper_bound": 231
             }
            ],
            "sketch": {
             "data": [
              [
               191,
               203,
               194,
               201,
               184,
               181,
               190,
               217,
               199,
               193,
               214,
               184,
               184,
               187,
               216,
               190,
               195,
               210,
               230,
               215,
               206,
               200,
               198,
               216,
               195,
               220,
               196,
               210,
               193,
               188,
               185,
               210,
               219,
               208,
               216,
               228,
               186,
               192,
               215,
               215,
               190,
               197,
               185,
               212,
               195,
               201,
               180,
               191,
               190,
               181,
               192,
               221,
               191,
               196,
               193,
               182,
               193,
               187,
               214,
               190,
               199,
               196,
               197,
               195,
               202,
               212,
               207,
               221,
               186,
               181,
               222,
               197,
               220,
               212,
               191,
               231,
               217,
               197,
               216,
               201,
               220,
               193,
               197,
               192,
               215,
               194,
               185,
               198,
               198,
               194,
               208,
               185,
               230,
               212,
               213,
               212,
               210,
               191,
               215,
               191,
               176,
               225,
               215,
               213,
               191,
               215,
               225,
               188,
               220,
               180,
               196,
               209,
               190,
               201,
               183,
               197,
               219,
               190,
               222,
               213,
               190,
               217,
               197,
               191,
               193,
               183,
               185,
               191,
               191,
               222,
               202,
               213,
               187,
               224,
               218,
               209,
               194,
               199,
               195,
               180,
               187,
               198,
               195,
               187,
               195,
               214,
               216,
               196,
               187,
               188,
               195,
               186,
               193,
               182,
               198,
               203,
               214,
               189,
               190,
               216,
               186,
               225,
               181,
               211,
               187,
               185,
               210,
               182,
               210,
               187,
               208,
               181,
               219,
               221,
               188,
               181,
               181,
               198,
               209,
               201,
               190,
               172,
               228,
               229,
               218,
               186,
               208,
               216,
               188,
               190,
               193,
               189,
               187,
               197,
               198,
               202,
               215,
               220,
               189,
               197,
               184,
               189,
               208,
               197,
               193,
               187,
               212,
               193,
               191,
               230,
               192,
               219,
               222,
               194,
               230,
               190,
               195,
               203,
               191,
               208,
               218,
               192,
               210,
               211,
               217,
               219,
               210,
               210,
               223,
               196,
               185,
               196,
               196,
               215,
               195,
               216,
               218,
               195
              ]
             ],
             "parameters": {
              "c": 0.64,
              "k": 2048
             }
            }
           }
          },
          "max": 231,
          "mean": 200.8361344537815,
          "min": 172,
          "std_dev": 13.65705549851377,
          "sum": 47799
         }
        },
        {
         "inferred_type": "Integral",
         "name": "groundtruth",
         "numerical_statistics": {
          "common": {
           "num_missing": 0,
           "num_present": 238
          },
          "distribution": {
           "kll": {
            "buckets": [
             {
              "count": 103,
              "lower_bound": 0,
              "upper_bound": 0.2
             },
             {
              "count": 0,
              "lower_bound": 0.2,
              "upper_bound": 0.4
             },
             {
              "count": 0,
              "lower_bound": 0.4,
              "upper_bound": 0.6
             },
             {
              "count": 0,
              "lower_bound": 0.6,
              "upper_bound": 0.8
             },
             {
              "count": 0,
              "lower_bound": 0.8,
              "upper_bound": 1
             },
             {
              "count": 48,
              "lower_bound": 1,
              "upper_bound": 1.2
             },
             {
              "count": 0,
              "lower_bound": 1.2,
              "upper_bound": 1.4
             },
             {
              "count": 0,
              "lower_bound": 1.4,
              "upper_bound": 1.6
             },
             {
              "count": 0,
              "lower_bound": 1.6,
              "upper_bound": 1.8
             },
             {
              "count": 87,
              "lower_bound": 1.8,
              "upper_bound": 2
             }
            ],
            "sketch": {
             "data": [
              [
               0,
               1,
               1,
               1,
               0,
               0,
               1,
               2,
               1,
               0,
               2,
               0,
               0,
               1,
               2,
               0,
               0,
               2,
               2,
               2,
               1,
               1,
               1,
               2,
               1,
               2,
               1,
               2,
               1,
               0,
               0,
               2,
               2,
               2,
               2,
               2,
               0,
               0,
               2,
               2,
               0,
               1,
               0,
               2,
               0,
               1,
               0,
               0,
               0,
               1,
               1,
               2,
               0,
               1,
               1,
               0,
               0,
               0,
               2,
               0,
               0,
               0,
               0,
               1,
               1,
               2,
               2,
               2,
               0,
               0,
               2,
               1,
               2,
               1,
               1,
               2,
               2,
               0,
               2,
               1,
               2,
               0,
               1,
               0,
               2,
               0,
               1,
               0,
               1,
               1,
               2,
               0,
               2,
               2,
               2,
               2,
               1,
               0,
               2,
               0,
               0,
               2,
               2,
               2,
               0,
               2,
               2,
               0,
               2,
               0,
               0,
               2,
               0,
               0,
               0,
               1,
               2,
               0,
               2,
               2,
               0,
               2,
               0,
               1,
               0,
               0,
               0,
               0,
               1,
               2,
               1,
               2,
               0,
               2,
               2,
               2,
               0,
               0,
               0,
               0,
               0,
               1,
               0,
               1,
               1,
               2,
               2,
               0,
               0,
               1,
               1,
               0,
               1,
               0,
               0,
               0,
               2,
               0,
               0,
               2,
               0,
               2,
               0,
               2,
               0,
               0,
               1,
               0,
               2,
               0,
               0,
               1,
               2,
               2,
               0,
               0,
               0,
               0,
               2,
               1,
               1,
               0,
               2,
               2,
               2,
               0,
               2,
               2,
               0,
               0,
               1,
               0,
               0,
               0,
               1,
               0,
               2,
               2,
               0,
               1,
               0,
               0,
               2,
               0,
               1,
               0,
               2,
               0,
               0,
               2,
               0,
               2,
               2,
               0,
               2,
               0,
               0,
               2,
               0,
               2,
               2,
               0,
               2,
               2,
               2,
               2,
               2,
               2,
               2,
               0,
               0,
               1,
               0,
               2,
               0,
               2,
               2,
               0
              ]
             ],
             "parameters": {
              "c": 0.64,
              "k": 2048
             }
            }
           }
          },
          "max": 2,
          "mean": 0.9327731092436975,
          "min": 0,
          "std_dev": 0.8909544729616281,
          "sum": 222
         }
        },
        {
         "inferred_type": "String",
         "name": "island",
         "string_statistics": {
          "common": {
           "num_missing": 0,
           "num_present": 238
          },
          "distinct_count": 3,
          "distribution": {
           "categorical": {
            "buckets": [
             {
              "count": 117,
              "value": "Biscoe"
             },
             {
              "count": 38,
              "value": "Torgersen"
             },
             {
              "count": 83,
              "value": "Dream"
             }
            ]
           }
          }
         }
        }
       ],
       "version": 0
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 167,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = f\"{data_quality_location}statistics.json\"\n",
    "JSON(json.loads(S3Downloader.read_file(statistics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c17076de-e3ba-4b0d-9472-d4d2dab542bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "features": [
        {
         "completeness": 1,
         "inferred_type": "Fractional",
         "name": "body_mass_g",
         "num_constraints": {
          "is_non_negative": true
         }
        },
        {
         "completeness": 1,
         "inferred_type": "Fractional",
         "name": "culmen_depth_mm",
         "num_constraints": {
          "is_non_negative": true
         }
        },
        {
         "completeness": 1,
         "inferred_type": "Fractional",
         "name": "culmen_length_mm",
         "num_constraints": {
          "is_non_negative": true
         }
        },
        {
         "completeness": 1,
         "inferred_type": "Fractional",
         "name": "flipper_length_mm",
         "num_constraints": {
          "is_non_negative": true
         }
        },
        {
         "completeness": 1,
         "inferred_type": "Integral",
         "name": "groundtruth",
         "num_constraints": {
          "is_non_negative": true
         }
        },
        {
         "completeness": 1,
         "inferred_type": "String",
         "name": "island",
         "string_constraints": {
          "domains": [
           "Biscoe",
           "Torgersen",
           "Dream"
          ]
         }
        }
       ],
       "monitoring_config": {
        "datatype_check_threshold": 1,
        "distribution_constraints": {
         "categorical_comparison_threshold": 0.1,
         "categorical_drift_method": "LInfinity",
         "comparison_method": "Robust",
         "comparison_threshold": 0.1,
         "perform_comparison": "Enabled"
        },
        "domain_content_threshold": 1,
        "emit_metrics": "Enabled",
        "evaluate_constraints": "Enabled"
       },
       "version": 0
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 168,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints = f\"{data_quality_location}constraints.json\"\n",
    "JSON(json.loads(S3Downloader.read_file(constraints)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a58ae-d3aa-4f69-bd52-66b05458bee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 7 - Scheduling the Monitoring Job\n",
    "\n",
    "We can now set up a schedule to continuously monitor data going into the endpoint and compare it to the baseline we generated before. This monitoring job will use the baseline statistics and constraints we generated during the Data Quality Check Step. Check [Schedule Data Quality Monitoring Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-schedule-data-monitor.html) for more information.\n",
    "\n",
    "SageMaker looks for violations in the data captured by the endpoint. By default, they combine the input data with the endpoint output and compare the result with the previous baseline we generated. If we let SageMaker do this, we will get three violations:\n",
    "\n",
    "1. An \"extra column check\" violation because the field `confidence` doesn't exist in the baseline.\n",
    "2. An \"extra column check\" violation because the field `prediction` doesn't exist in the baseline.\n",
    "3. A \"missing column check\" violation because the field `groundtruth` doesn't appear in the data captured from the endpoint.\n",
    "\n",
    "We can fix these violations by creating a preprocessing script configuring the data we want the monitoring job to use. This script will create a `groundtruth` column, and exclude `confidence` and `prediction`. By doing this, we will not receive any of these three violations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "345efa6b-40bd-4084-83fd-654847bf22c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_QUALITY_PREPROCESSOR = \"data_quality_preprocessor.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c83ca-9b2c-40cc-81f9-bf85c0f17b5e",
   "metadata": {},
   "source": [
    "Here is the preprocessing script for the Data Quality Monitoring Job. Check [Preprocessing and Postprocessing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html) for more information about how to configure these scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2efcc106-b86d-4998-8fae-095c52a2b56b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguins/data_quality_preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PENGUINS_FOLDER}/{DATA_QUALITY_PREPROCESSOR}\n",
    "import json\n",
    "\n",
    "def preprocess_handler(inference_record):\n",
    "    input_data = inference_record.endpoint_input.data\n",
    "    output_data = json.loads(inference_record.endpoint_output.data)\n",
    "    \n",
    "    response = json.loads(input_data)\n",
    "    response[\"groundtruth\"] = output_data[\"prediction\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa310b-2443-4a61-a09a-ea59df21f5c6",
   "metadata": {},
   "source": [
    "The monitoring schedule expects an S3 location pointing to the preprocessing script. Let's upload the script to the default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f501b907-6844-412e-bb79-e04fce790e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-325223348818/penguins-monitoring/data_quality_preprocessor.py'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = boto3.Session().resource(\"s3\").Bucket(sagemaker_session.default_bucket())\n",
    "prefix = \"penguins-monitoring\"\n",
    "bucket.Object(os.path.join(prefix, DATA_QUALITY_PREPROCESSOR)).upload_file(str(PENGUINS_FOLDER / DATA_QUALITY_PREPROCESSOR))\n",
    "data_quality_preprocessor = f\"s3://{os.path.join(bucket.name, prefix, DATA_QUALITY_PREPROCESSOR)}\"\n",
    "data_quality_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c8199-5c13-4284-9ca9-1b695e3ff122",
   "metadata": {},
   "source": [
    "We can now set up the Data Quality Monitoring Job using the [DefaultModelMonitor](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_monitor.model_monitoring.DefaultModelMonitor) class. Notice how we specify the `record_preprocessor_script` using the S3 location where we uploaded our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5ded982d-d858-4ca6-b754-196917251a60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: penguins-data-monitoring-schedule\n"
     ]
    }
   ],
   "source": [
    "data_monitor = DefaultModelMonitor(\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "data_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"penguins-data-monitoring-schedule\",\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    record_preprocessor_script=data_quality_preprocessor,\n",
    "    statistics=f\"{data_quality_location}statistics.json\",\n",
    "    constraints=f\"{data_quality_location}constraints.json\",\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3705ec7-e7fb-4eca-8290-53df06361e77",
   "metadata": {},
   "source": [
    "You can describe the schedule to see more information about the Data Quality Monitoring Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2e6ce5ec-a4c4-4f79-a78b-d27f678f6db9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-east-1:325223348818:monitoring-schedule/penguins-data-monitoring-schedule',\n",
       " 'MonitoringScheduleName': 'penguins-data-monitoring-schedule',\n",
       " 'MonitoringScheduleStatus': 'Scheduled',\n",
       " 'MonitoringType': 'DataQuality',\n",
       " 'CreationTime': datetime.datetime(2023, 6, 6, 14, 37, 29, 193000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 6, 6, 16, 18, 46, 82000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinitionName': 'data-quality-job-definition-2023-06-06-14-37-28-130',\n",
       "  'MonitoringType': 'DataQuality'},\n",
       " 'EndpointName': 'penguins-endpoint',\n",
       " 'LastMonitoringExecutionSummary': {'MonitoringScheduleName': 'penguins-data-monitoring-schedule',\n",
       "  'ScheduledTime': datetime.datetime(2023, 6, 6, 16, 0, tzinfo=tzlocal()),\n",
       "  'CreationTime': datetime.datetime(2023, 6, 6, 16, 7, 31, 229000, tzinfo=tzlocal()),\n",
       "  'LastModifiedTime': datetime.datetime(2023, 6, 6, 16, 18, 46, 69000, tzinfo=tzlocal()),\n",
       "  'MonitoringExecutionStatus': 'Completed',\n",
       "  'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:325223348818:processing-job/model-monitoring-202306061600-06f7393fd92f4043f1f9784f',\n",
       "  'EndpointName': 'penguins-endpoint'},\n",
       " 'ResponseMetadata': {'RequestId': 'e78a25a6-76c8-4a60-81f5-8a8704362363',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e78a25a6-76c8-4a60-81f5-8a8704362363',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '959',\n",
       "   'date': 'Tue, 06 Jun 2023 16:22:46 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cb2fa-0947-479a-b6be-969b0878cb6f",
   "metadata": {},
   "source": [
    "## Step 8 - Introducing Drift\n",
    "\n",
    "Let's make a prediction for a penguin with a body mass larger than what we've seen before. This prediction should get flagged by the monitoring job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c4f88fd4-ceae-4b1f-9085-b7cf54bd899f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': '0', 'confidence': 0.999999642}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "    \"island\": \"Dream\",\n",
    "    \"culmen_length_mm\": 46.4,\n",
    "    \"culmen_depth_mm\": 18.6,\n",
    "    \"flipper_length_mm\": 190.0,\n",
    "    \n",
    "    # This body mass is outside of the range we saw\n",
    "    # in the train set.\n",
    "    \"body_mass_g\": 15000.0,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1f841-8962-436c-9766-0359a709747b",
   "metadata": {},
   "source": [
    "## Step 9 - Cleaning up\n",
    "\n",
    "Let's stop the monitoring jobs by deleting the monitoring schedule we created before. The following function waits for the job to finish before deleting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85cf8e-0d56-483c-ae68-385cf79ac0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_monitoring_schedule(schedule):\n",
    "    attempts = 30\n",
    "    \n",
    "    try:\n",
    "        status = schedule.describe_schedule()[\"MonitoringScheduleStatus\"]\n",
    "    except Exception:\n",
    "        print(\"Monitoring schedule deleted.\")\n",
    "        return\n",
    "        \n",
    "    while status in (\"Pending\", \"InProgress\") and attempts > 0:\n",
    "        attempts -= 1\n",
    "        print(f\"Monitoring schedule status: {status}. Waiting for it to finish.\")\n",
    "        time.sleep(30)\n",
    "        status = schedule.describe_schedule()[\"MonitoringScheduleStatus\"]\n",
    "\n",
    "    if status not in (\"Pending\", \"InProgress\"):\n",
    "        schedule.delete_monitoring_schedule()\n",
    "        print(\"Monitoring schedule deleted.\")\n",
    "    else:\n",
    "        print(\"Waiting for monitoring schedule timed out\")\n",
    "\n",
    "\n",
    "delete_monitoring_schedule(data_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91729f95-4966-4741-aa74-2d90382710ea",
   "metadata": {},
   "source": [
    "Let's now stop the thread generating traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec787f-4ee7-4620-ac2b-a77c977a2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_traffic_thread.set()\n",
    "traffic_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776d472-323f-4205-9e6d-076375323280",
   "metadata": {},
   "source": [
    "Finally, we can delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63614880-de4b-4cf9-bf1b-3f141d881559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: penguins-endpoint-config-0606132701\n",
      "INFO:sagemaker:Deleting endpoint with name: penguins-endpoint\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917fe920-172d-4b07-bd7d-b2ecea536319",
   "metadata": {},
   "source": [
    "# Session 6 - Model Monitoring\n",
    "\n",
    "This session aims to set up a monitoring process to analyze the quality of the model predictions. For this, we need to generate ground truth for the data captured by the endpoint and compare it with a baseline performance.\n",
    "\n",
    "Check [Amazon SageMaker Model Monitor](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_monitoring.html) for a brief explanation of how to use SageMaker's Model Monitoring functionality. [Monitor models for data and model quality, bias, and explainability](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html) is a much more extensive guide to Model Monitoring in Amazon SageMaker.\n",
    "\n",
    "Here is what the Pipeline will look like at the end of this session:\n",
    "\n",
    "<img src='penguins/images/session6-pipeline.png' alt='Session 6 Pipeline' width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "748c47c5-71e6-4f76-9a8b-9a5f4716e806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.quality_check_step import ModelQualityCheckConfig\n",
    "\n",
    "from sagemaker.inputs import CreateModelInput, TransformInput\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.steps import CreateModelStep, TransformStep\n",
    "from sagemaker.model_monitor import ModelQualityMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81430dfd-2524-43e4-bfe9-c6545316005d",
   "metadata": {},
   "source": [
    "## Step 1 - Creating Test Predictions\n",
    "\n",
    "To create a baseline to compare the model performance, we must create predictions for the test set and compare them with the predictions from the model. We can do this by running a Batch Transform Job to predict every sample from the test dataset. We can use a [Transform Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-transform) as part of the pipeline to run this job. You can check [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) for more information about Batch Transform Jobs.\n",
    "\n",
    "The Transform Step requires a model to generate predictions, so we need a Model Step that creates a model.\n",
    "\n",
    "We also need to configure the [Batch Transform Job](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) using a [Transform Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-transform). This Batch Transform Job will run every sample from the training dataset through the model so we can compute the baseline metrics. We can use an instance of the [Transformer](https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html) class to configure the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1987a788-de7a-4f60-ac8d-819d9ffcdf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_model_step = ModelStep(\n",
    "    name=\"create-model\",\n",
    "    step_args=model.create(instance_type=\"ml.m5.large\"),\n",
    ")\n",
    "\n",
    "transformer = Transformer(\n",
    "    # The Batch Transform Job will use the model we created using the\n",
    "    # Model Step.\n",
    "    model_name=create_model_step.properties.ModelName,\n",
    "    \n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    instance_count=1,\n",
    "    \n",
    "    # The baseline set that we generated in the preprocessing step\n",
    "    # is in JSON format, where every line is a JSON sample.\n",
    "    accept=\"application/json\",\n",
    "    strategy=\"SingleRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    \n",
    "    output_path=f\"{S3_FILEPATH}/transform\",\n",
    ")\n",
    "\n",
    "generate_test_predictions_step = TransformStep(\n",
    "    name=\"generate_test_predictions\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(\n",
    "        \n",
    "        # We will use the test dataset we generated during the preprocessing \n",
    "        # step to run it through the model and generate predictions.\n",
    "        data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\"test-baseline\"].S3Output.S3Uri,\n",
    "\n",
    "        join_source=\"Input\",\n",
    "        content_type=\"application/json\",\n",
    "        split_type=\"Line\",\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafc7c4-6fef-4832-8b99-8c45d078fdd2",
   "metadata": {},
   "source": [
    "## Step 2 - Generating a Baseline\n",
    "\n",
    "Let's now configure the [Quality Check Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-quality-check) and feed it the data we generated in the Transform Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9aa3a284-8763-4000-a263-70314b530652",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "model_quality_location = f\"{S3_FILEPATH}/monitoring/model-quality/\"\n",
    "\n",
    "model_quality_baseline_step = QualityCheckStep(\n",
    "    name=\"generate-model-quality-baseline\",\n",
    "    \n",
    "    check_job_config = CheckJobConfig(\n",
    "        instance_type=\"ml.t3.xlarge\",\n",
    "        instance_count=1,\n",
    "        volume_size_in_gb=20,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    ),\n",
    "    \n",
    "    quality_check_config = ModelQualityCheckConfig(\n",
    "        # We are going to use the output of the Transform Step to generate\n",
    "        # the model quality baseline.\n",
    "        baseline_dataset=generate_test_predictions_step.properties.TransformOutput.S3OutputPath,\n",
    "\n",
    "        dataset_format=DatasetFormat.json(lines=True),\n",
    "        # output_s3_uri=Join(on='/', values=[S3_FILEPATH, \"monitoring\", \"model-quality\"]),\n",
    "        output_s3_uri=model_quality_location,\n",
    "\n",
    "        # We need to specify the problem type and the fields where the prediction\n",
    "        # and groundtruth are so the process knows how to interpret the results.\n",
    "        problem_type=\"MulticlassClassification\",\n",
    "        inference_attribute=\"$.SageMakerOutput.prediction\",\n",
    "        ground_truth_attribute=\"groundtruth\",\n",
    "    ),\n",
    "    \n",
    "    skip_check=True,\n",
    "    register_new_baseline=True,\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693535ba-fca7-4e89-a4cb-b4f333fa2d03",
   "metadata": {},
   "source": [
    "## Step 3 - Setting up Model Metrics\n",
    "\n",
    "We can configure a new set of [ModelMetrics](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_metrics.ModelMetrics) using the results of the Data and Model Quality Steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a773f134-ac2f-4dba-976e-9b7f0b384b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_data_statistics=MetricsSource(\n",
    "        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_data_constraints=MetricsSource(\n",
    "        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    \n",
    "    model_constraints=MetricsSource(\n",
    "        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# drift_check_baselines = DriftCheckBaselines(\n",
    "#     model_data_statistics=MetricsSource(\n",
    "#         s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n",
    "#         content_type=\"application/json\",\n",
    "#     ),\n",
    "#     model_data_constraints=MetricsSource(\n",
    "#         s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "#         content_type=\"application/json\",\n",
    "#     ),\n",
    "#     model_statistics=MetricsSource(\n",
    "#         s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n",
    "#         content_type=\"application/json\",\n",
    "#     ),\n",
    "#     model_constraints=MetricsSource(\n",
    "#         s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "#         content_type=\"application/json\",\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3487a0-05ad-4f3a-8f50-9884dc2aef64",
   "metadata": {},
   "source": [
    "## Step 4 - Registering the Model\n",
    "\n",
    "We need to redefine the Model Step to register the [TensorFlowModel](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-serving-model) so it takes into account the new metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7056a009-91c0-4955-90dd-b90ef8cab149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "register_model_step = ModelStep(\n",
    "    name=\"register-model\",\n",
    "    step_args=model.register(\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        model_metrics=model_metrics,\n",
    "        # drift_check_baselines=drift_check_baselines,\n",
    "        approval_status=\"Approved\",\n",
    "\n",
    "        content_types=[\"application/json\"],\n",
    "        response_types=[\"application/json\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"TENSORFLOW\",\n",
    "        framework_version=\"2.6\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00b5e6-9858-4acc-bbfe-a2ce24ec20e0",
   "metadata": {},
   "source": [
    "## Step 5 - Setting up the Condition Step\n",
    "\n",
    "We only want to compute the model quality baseline if the model's performance is above the predefined threshold. The [Condition Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition) will gate all necessary steps to compute the baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bacaa9c6-22b0-48df-b138-95b6422fe834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition_step = ConditionStep(\n",
    "    name=\"check-model-accuracy\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[\n",
    "        create_model_step, \n",
    "        generate_test_predictions_step, \n",
    "        model_quality_baseline_step, \n",
    "        register_model_step,\n",
    "        deploy_step\n",
    "    ],\n",
    "    else_steps=[fail_step], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a7905-2550-4979-b885-f2daabb5d45e",
   "metadata": {},
   "source": [
    "## Step 6 - Running the Pipeline\n",
    "\n",
    "We can now run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4da5e453-acd8-47a0-a39f-264d05dd93d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session6_pipeline = Pipeline(\n",
    "    name=\"penguins-session6-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location, \n",
    "        preprocessor_destination,\n",
    "        train_dataset_baseline_destination,\n",
    "        test_dataset_baseline_destination,\n",
    "        timestamp_signature,\n",
    "        data_capture_percentage,\n",
    "        data_capture_destination,\n",
    "       \n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "        data_quality_baseline_step,\n",
    "        train_model_step, \n",
    "        evaluate_model_step,\n",
    "        condition_step\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24713751-b109-47be-8d6f-5f4c7511f6d4",
   "metadata": {},
   "source": [
    "Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b54dbf6e-9b52-41a6-bec3-91c56536b3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "session6_pipeline.upsert(role_arn=role)\n",
    "execution = session6_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef99386-e0f3-411d-8715-8538d6d7bba9",
   "metadata": {},
   "source": [
    "## Step 7 - Setting Up a Predictor\n",
    "\n",
    "We can now create a [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) from the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfd679a8-7f4b-47ee-9bdc-ca9942aa59a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "waiter = sagemaker_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={\n",
    "        \"Delay\": 10,\n",
    "        \"MaxAttempts\": 30\n",
    "    }\n",
    ")\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b479872-58e2-4c61-8ccf-68d35185b282",
   "metadata": {},
   "source": [
    "## Step 8 - Generating Ground Truth Data\n",
    "\n",
    "Let's generate predictions and ground truth data to test the monitoring functionality.\n",
    "\n",
    "We will repeatedly send every sample from the dataset to the endpoint and generate random ground truth data for the sake of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee5d6f-af93-41e8-a544-3a12dac99d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = f\"{S3_FILEPATH}/monitoring/groundtruth\" \n",
    "\n",
    "def generate_ground_truth_data(ground_truth_path):\n",
    "    \n",
    "    def _generate_ground_truth_record(inference_id):\n",
    "        random.seed(inference_id)\n",
    "\n",
    "        return {\n",
    "            \"groundTruthData\": {\n",
    "                \"data\": str(random.choice([0, 1, 2])),\n",
    "                \"encoding\": \"CSV\",\n",
    "            },\n",
    "            \"eventMetadata\": {\n",
    "                \"eventId\": str(inference_id),\n",
    "            },\n",
    "            \"eventVersion\": \"0\",\n",
    "        }\n",
    "\n",
    "\n",
    "    def _upload_ground_truth(records, upload_time):\n",
    "        records = [json.dumps(r) for r in records]\n",
    "        data = \"\\n\".join(records)\n",
    "        uri = f\"{ground_truth_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "\n",
    "        print(f\"Uploading ground truth data to {uri}...\")\n",
    "\n",
    "        S3Uploader.upload_string_as_file_body(data, uri)    \n",
    "\n",
    "                \n",
    "    def _generate_ground_truth_data(max_records, stop_ground_truth_thread):\n",
    "        while True:\n",
    "            records = [_generate_ground_truth_record(i) for i in range(max_records)]\n",
    "            _upload_ground_truth(records, datetime.utcnow())\n",
    "\n",
    "            if stop_ground_truth_thread.is_set():\n",
    "                break\n",
    "\n",
    "            sleep(30)\n",
    "\n",
    "                \n",
    "    stop_ground_truth_thread = Event()\n",
    "    data = pd.read_csv(LOCAL_FILEPATH).dropna()\n",
    "    \n",
    "    groundtruth_thread = Thread(\n",
    "        target=_generate_ground_truth_data,\n",
    "        args=(len(data), stop_ground_truth_thread,)\n",
    "    )\n",
    "    \n",
    "    groundtruth_thread.start()\n",
    "    \n",
    "    return stop_ground_truth_thread, traffic_thread\n",
    "\n",
    "\n",
    "stop_ground_truth_thread, groundtruth_thread = generate_ground_truth_data(ground_truth_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beba47c-75bf-49fd-80e9-146c3ff815f1",
   "metadata": {},
   "source": [
    "Let's generate prediction data using the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f586a-eb71-4d90-b2cd-c59c2fbac70c",
   "metadata": {},
   "source": [
    "Let's generate ground truth data and store it in S3. Notice the S3 path to save the ground truth data has the same path format as the data captured by the endpoint. Check [Ingest Ground Truth Labels and Merge Them With Predictions](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-merge.html) for more information about this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f37ae-7656-4b8d-8c7d-60f9fa5c737d",
   "metadata": {},
   "source": [
    "Let's start both threads now. These threads will run forever until you restart the kernel or set the `stop_thread` event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b40a0087-93d7-4b6e-9be5-d7a2c622b77c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ground truth data to s3://mlschool/penguins/monitoring/groundtruth/2023/05/26/17/5032.jsonl...\n"
     ]
    }
   ],
   "source": [
    "stop_traffic_thread, traffic_thread = generate_traffic()\n",
    "stop_ground_truth_thread, groundtruth_thread = generate_ground_truth_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecc695-535a-4556-89e6-082670746afa",
   "metadata": {},
   "source": [
    "## Step 13 - Running the Monitoring Job\n",
    "\n",
    "Let's set up a schedule to continuously monitor the quality of the model and compare it to the baseline we generated before. This monitoring job will use the baseline constraints we generated during the Model Quality Check Step. Check [Schedule Model Quality Monitoring Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-schedule.html) for more information.\n",
    "\n",
    "To set up a Model Quality Monitoring Job, we can use the [ModelQualityMonitor](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_monitor.model_monitoring.ModelQualityMonitor) class. The [EndpointInput](https://sagemaker.readthedocs.io/en/v2.24.2/api/inference/model_monitor.html#sagemaker.model_monitor.model_monitoring.EndpointInput) instance configures the attribute the monitoring job should use to determine the prediction from the model.\n",
    "\n",
    "Check [Amazon SageMaker Model Quality Monitor](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model_monitor/model_quality/model_quality_churn_sdk.html) for a complete tutorial on how to run a Model Monitoring Job in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6b1bfce1-e812-4c6e-bade-8419d6c7ae5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: penguins-model-monitoring-schedule\n"
     ]
    }
   ],
   "source": [
    "model_monitor = ModelQualityMonitor(\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "endpoint_input = EndpointInput(\n",
    "    endpoint_name=predictor.endpoint_name,\n",
    "    \n",
    "    # The endpoint returns an attribute `prediction` with the\n",
    "    # prediction from the model. That's the attribute we want to\n",
    "    # use to compare with the ground truth.\n",
    "    inference_attribute=\"prediction\",\n",
    "    \n",
    "    destination=\"/opt/ml/processing/input_data\",\n",
    ")\n",
    "\n",
    "model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"penguins-model-monitoring-schedule\",\n",
    "    endpoint_input=endpoint_input,\n",
    "    problem_type=\"MulticlassClassification\",\n",
    "    \n",
    "    ground_truth_input=ground_truth_path,\n",
    "    \n",
    "    constraints=f\"{model_quality_location}constraints.json\",\n",
    "    \n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    output_s3_uri=f\"{S3_FILEPATH}/monitoring/model-quality\",\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3e732-039b-471d-bfa9-8dd8b39950dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can describe the schedule to see more information about the Model Quality Monitoring Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "126bfac0-09a2-44a8-a6e3-b43abe9d9983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-east-1:325223348818:monitoring-schedule/penguins-model-monitoring-schedule',\n",
       " 'MonitoringScheduleName': 'penguins-model-monitoring-schedule',\n",
       " 'MonitoringScheduleStatus': 'Scheduled',\n",
       " 'MonitoringType': 'ModelQuality',\n",
       " 'CreationTime': datetime.datetime(2023, 5, 26, 20, 37, 1, 422000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 5, 26, 21, 19, 5, 921000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinitionName': 'model-quality-job-definition-2023-05-26-20-37-00-772',\n",
       "  'MonitoringType': 'ModelQuality'},\n",
       " 'EndpointName': 'penguins-endpoint',\n",
       " 'LastMonitoringExecutionSummary': {'MonitoringScheduleName': 'penguins-model-monitoring-schedule',\n",
       "  'ScheduledTime': datetime.datetime(2023, 5, 26, 21, 0, tzinfo=tzlocal()),\n",
       "  'CreationTime': datetime.datetime(2023, 5, 26, 21, 0, 56, 245000, tzinfo=tzlocal()),\n",
       "  'LastModifiedTime': datetime.datetime(2023, 5, 26, 21, 19, 5, 905000, tzinfo=tzlocal()),\n",
       "  'MonitoringExecutionStatus': 'CompletedWithViolations',\n",
       "  'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:325223348818:processing-job/model-quality-monitoring-202305262100-15591e30a6d45da2c2b604eb',\n",
       "  'EndpointName': 'penguins-endpoint'},\n",
       " 'ResponseMetadata': {'RequestId': '76947156-7cf1-4cb3-ad4a-5197ef2bea17',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '76947156-7cf1-4cb3-ad4a-5197ef2bea17',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '987',\n",
       "   'date': 'Fri, 26 May 2023 21:22:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dded2c0-2ef4-4a46-bc79-8366998aa8c4",
   "metadata": {},
   "source": [
    "## Step 14 - Stopping Monitoring Jobs\n",
    "\n",
    "Let's stop the monitoring jobs by deleting the monitoring schedules we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7b1ada75-1606-4980-8fd5-620dfe9c6608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring schedule deleted.\n",
      "Monitoring schedule deleted.\n"
     ]
    }
   ],
   "source": [
    "delete_monitoring_schedule(model_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f0425-f0a4-4530-851b-7f665c55106b",
   "metadata": {},
   "source": [
    "We also need to stop the threads generating predictions and ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f265d3ab-384e-4e49-ab94-e7f87dea8f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ground truth data to s3://mlschool/penguins/monitoring/groundtruth/2023/05/26/21/2256.jsonl...\n"
     ]
    }
   ],
   "source": [
    "stop_traffic_thread.set()\n",
    "stop_ground_truth_thread.set()\n",
    "\n",
    "traffic_thread.join()\n",
    "groundtruth_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad4f27-8ddb-44c8-b472-08fc69eab34e",
   "metadata": {},
   "source": [
    "## Step 15 - Cleaning up\n",
    "\n",
    "Before you finish, don't forget to clean up after yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "12ec02ec-6de1-4973-b9a0-525c81a5a56e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: penguins-endpoint-config-0526171418\n",
      "INFO:sagemaker:Deleting endpoint with name: penguins-endpoint\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0335ed-643d-4755-a1b2-ce129a8d7730",
   "metadata": {},
   "source": [
    "## Final Clean Up\n",
    "\n",
    "Here we can do a more deep clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4cda93bb-29f8-47ad-a25f-f7b21df90325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline):\n",
    "    if pipeline:\n",
    "        pipeline.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278e6f6-4001-4865-9ddb-6535c5dff961",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_pipeline(session1_pipeline)\n",
    "delete_pipeline(session2_pipeline)\n",
    "delete_pipeline(session3_pipeline)\n",
    "delete_pipeline(session4_pipeline)\n",
    "delete_pipeline(session5_pipeline)\n",
    "delete_pipeline(session6_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8704c94-7a42-45f9-b006-74832f07cbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f8f8d763-787e-4474-88b4-d4b4894fd923',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f8f8d763-787e-4474-88b4-d4b4894fd923',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 06 Jun 2023 13:31:20 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's delete every model we registered under our model package group\n",
    "for mp in sagemaker_client.list_model_packages(ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"]:\n",
    "    print(f\"Deleting {mp['ModelPackageArn']}\")\n",
    "    sagemaker_client.delete_model_package(ModelPackageName=mp[\"ModelPackageArn\"])\n",
    "\n",
    "# We can now delete the model package group.    \n",
    "sagemaker_client.delete_model_package_group(ModelPackageGroupName=model_package_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b535b3da-c6ff-480e-ac30-bdeb00f209ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_pipeline(session5_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119eb67b-cd5e-458e-845a-6a07e512cf67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
